{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is Maxwell's daemon, an application that reads MySQL binlogs and writes row updates as JSON to Kafka, Kinesis, or other streaming platforms. Maxwell has low operational overhead, requiring nothing but mysql and a place to write to. Its common use cases include ETL, cache building/expiring, metrics collection, search indexing and inter-service communication. Maxwell gives you some of the benefits of event sourcing without having to re-architect your entire platform. Download: https://github.com/zendesk/maxwell/releases/download/v1.31.0/maxwell-1.31.0.tar.gz Source: https://github.com/zendesk/maxwell mysql> insert into `test`.`maxwell` set id = 1, daemon = 'Stanislaw Lem'; maxwell: { \"database\": \"test\", \"table\": \"maxwell\", \"type\": \"insert\", \"ts\": 1449786310, \"xid\": 940752, \"commit\": true, \"data\": { \"id\":1, \"daemon\": \"Stanislaw Lem\" } } mysql> update test.maxwell set daemon = 'firebus! firebus!' where id = 1; maxwell: { \"database\": \"test\", \"table\": \"maxwell\", \"type\": \"update\", \"ts\": 1449786341, \"xid\": 940786, \"commit\": true, \"data\": {\"id\":1, \"daemon\": \"Firebus! Firebus!\"}, \"old\": {\"daemon\": \"Stanislaw Lem\"} }","title":"Home"},{"location":"bootstrapping/","text":"Bootstrapping Maxwell allows you to \"bootstrap\" data into your stream. This will perform a select * from table and output the results into your stream, allowing you to recreate your entire dataset by playing the stream from the start. Using the maxwell-bootstrap utility You can use the maxwell-bootstrap utility to begin boostrap operations from the command-line. option description --log_level LOG_LEVEL log level (DEBUG, INFO, WARN or ERROR) --user USER mysql username --password PASSWORD mysql password --host HOST mysql host --port PORT mysql port --database DATABASE mysql database containing the table to bootstrap --table TABLE mysql table to bootstrap --where WHERE_CLAUSE where clause to restrict the rows bootstrapped from the specified table --client_id CLIENT_ID specify which maxwell instance should perform the bootstrap operation --comment COMMENT arbitrary comment to be added to every bootstrap row record Starting a table bootstrap You can start a bootstrap using: bin/maxwell-bootstrap --database fooDB --table barTable Optionally, you can include a where clause to replay part of the data. bin/maxwell-bootstrap --database fooDB --table barTable --where \"my_date >= '2017-01-07 00:00:00'\" Alternatively you can insert a row in the maxwell.bootstrap table to trigger a bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable'); Note that if a Maxwell client_id has been set you should specify the client id. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id) values ('fooDB', 'barTable', 'custom_maxwell_client_id'); You can schedule bootstrap tasks to be run in the future by setting the started_at column. Maxwell will wait until this time to start the bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id, started_at) values ('fooDB', 'barTable', 'custom_maxwell_client_id', '2020-05-18 12:30:00'); Async vs Sync bootstrapping The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time. When running Maxwell with --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete. Running Maxwell with --bootstrapper=async however, will make Maxwell spawn a separate thread for bootstrapping. In this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process. Bootstrapping Data Format a bootstrap starts with an event of type = \"bootstrap-start\" then events with type = \"bootstrap-insert\" (one per row in the table) then one event per INSERT , UPDATE or DELETE with standard event types i.e. type = \"insert\" , type = \"update\" or type = \"delete\" that occurred since the beginning of bootstrap finally an event with type = \"bootstrap-complete\" Here's a complete example: mysql> create table fooDB.barTable(txt varchar(255)); mysql> insert into fooDB.barTable (txt) values (\"hello\"), (\"bootstrap!\"); mysql> insert into maxwell.bootstrap (database_name, table_name) values (\"fooDB\", \"barTable\"); Corresponding replication stream output of table fooDB.barTable : {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-start\",\"ts\":1450557744,\"data\":{}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-complete\",\"ts\":1450557744,\"data\":{}} Failure Scenarios If Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress. If this behavior is not desired, manual updates to the bootstrap table are required. Specifically, marking the unfinished bootstrap row as 'complete' ( is_complete = 1) or deleting the row. jQuery(document).ready(function () { jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\"); });","title":"Bootstrapping"},{"location":"bootstrapping/#bootstrapping","text":"Maxwell allows you to \"bootstrap\" data into your stream. This will perform a select * from table and output the results into your stream, allowing you to recreate your entire dataset by playing the stream from the start.","title":"Bootstrapping"},{"location":"bootstrapping/#using-the-maxwell-bootstrap-utility","text":"You can use the maxwell-bootstrap utility to begin boostrap operations from the command-line. option description --log_level LOG_LEVEL log level (DEBUG, INFO, WARN or ERROR) --user USER mysql username --password PASSWORD mysql password --host HOST mysql host --port PORT mysql port --database DATABASE mysql database containing the table to bootstrap --table TABLE mysql table to bootstrap --where WHERE_CLAUSE where clause to restrict the rows bootstrapped from the specified table --client_id CLIENT_ID specify which maxwell instance should perform the bootstrap operation --comment COMMENT arbitrary comment to be added to every bootstrap row record","title":"Using the maxwell-bootstrap utility"},{"location":"bootstrapping/#starting-a-table-bootstrap","text":"You can start a bootstrap using: bin/maxwell-bootstrap --database fooDB --table barTable Optionally, you can include a where clause to replay part of the data. bin/maxwell-bootstrap --database fooDB --table barTable --where \"my_date >= '2017-01-07 00:00:00'\" Alternatively you can insert a row in the maxwell.bootstrap table to trigger a bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable'); Note that if a Maxwell client_id has been set you should specify the client id. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id) values ('fooDB', 'barTable', 'custom_maxwell_client_id'); You can schedule bootstrap tasks to be run in the future by setting the started_at column. Maxwell will wait until this time to start the bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id, started_at) values ('fooDB', 'barTable', 'custom_maxwell_client_id', '2020-05-18 12:30:00');","title":"Starting a table bootstrap"},{"location":"bootstrapping/#async-vs-sync-bootstrapping","text":"The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time. When running Maxwell with --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete. Running Maxwell with --bootstrapper=async however, will make Maxwell spawn a separate thread for bootstrapping. In this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.","title":"Async vs Sync bootstrapping"},{"location":"bootstrapping/#bootstrapping-data-format","text":"a bootstrap starts with an event of type = \"bootstrap-start\" then events with type = \"bootstrap-insert\" (one per row in the table) then one event per INSERT , UPDATE or DELETE with standard event types i.e. type = \"insert\" , type = \"update\" or type = \"delete\" that occurred since the beginning of bootstrap finally an event with type = \"bootstrap-complete\" Here's a complete example: mysql> create table fooDB.barTable(txt varchar(255)); mysql> insert into fooDB.barTable (txt) values (\"hello\"), (\"bootstrap!\"); mysql> insert into maxwell.bootstrap (database_name, table_name) values (\"fooDB\", \"barTable\"); Corresponding replication stream output of table fooDB.barTable : {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-start\",\"ts\":1450557744,\"data\":{}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-complete\",\"ts\":1450557744,\"data\":{}}","title":"Bootstrapping Data Format"},{"location":"bootstrapping/#failure-scenarios","text":"If Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress. If this behavior is not desired, manual updates to the bootstrap table are required. Specifically, marking the unfinished bootstrap row as 'complete' ( is_complete = 1) or deleting the row. jQuery(document).ready(function () { jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\"); });","title":"Failure Scenarios"},{"location":"changelog/","text":"Maxwell changelog v1.31.0 : \"84 tent cabin\" Add producer for NATS streaming server v1.30.0 : \"all of this has happened before\" support server-sent heartbeating on the binlog connection via --binlog-heartbeat can connect to rabbitmq by URL, supports SSL connections fix parser bug with multiline SQL target JDK 11 -- we have dropped support for JDK 8 ability to send a microsecond timestamp via --output_push_timestamp fixes for odd azure mysql connection failures v1.29.2 : \"i now know the meaning of shame\" fix for terrible performance regression in bootstrapping v1.29.1 : \"depluralize\" small bugfix release, fixes binlog event type processing in mysql 8 v1.29.0 : \"i don't know, i don't know, i don't know\" High Availability support via jgroups-raft rework --help text v1.28.2 : \"fantasy baseball\" fix for encryption parsing error on table creation some logging around memory usage in RowMapBuffer v1.28.1 : \"bootras bootras gallliiiii\" fix http server issue in 1.28.0 v1.28.0 : \"stardew mania\" schema compaction! with the new --max_schemas option, maxwell will periodically roll up the maxwell . schemas table, preventing it from growing infinitely long. fix metricsAgeSloMS calculation support SRID columns fix parsing of complex INDEX(CAST()) statements various dependency bumps v1.27.1 : \"red bag? red bag\" redis producer gets sentinal support fix a double-reconnect race condition file producer honors javascript row-suppression better error messaging when we lack REPLICATION SLAVE privs miscellaneous dependency bumps v1.27.0 : \"running water\" better support for empty/null passwords allow bootstrap utility to query replication_host a few library upgrades, notably pubsub and kinesis library bootstrap connection uses jdbc_options properly add logging for when we hit out of sync schema exceptions allow for partitioning by thread_id, thx @gogov fresh and clean documentation v1.26.4 : \"No songs here\" support now() function with precision v1.26.3 : \"the worst song in the goddamn world\" use pooled redis connections, fixes corruption when redis was accessed from multiple threads (bootstrap/producer), thanks @lucastex fix date handling of '0000-01-01' fix race condition in binlog reconnect logic v1.26.2 : \"dave the butcher\" bootstraps can be scheduled in the future by setting the started_at column, thanks @lucastex two mysql 8 fixes; one for a DEFAULT(function()) parse error, one for supporting DEFAULT ENCRYPTION v1.26.1 : \"maybe we can break your ankle / clean and unsuspiciously\" fixes for redis re-connection login, thanks much @lucastex v1.26.0 : \"tip the waitress, feed her cocaine habit\" We now support mysql 8's caching_sha2_password authentication scheme support for converting JSON field names to camelCase v1.25.3 : \"bye, bolinas\" fixes memory leak in mysql-binlog-connector fixes exceptions that occur when a connection passes wait_timeout v1.25.2 : \"love potion #9\" Fixes for a long standing JSON bug in 8.0.19+ v1.25.1 : \"nowhere to put it\" issue #1457, ALTER DATABASE with implicit database name maxwell now runs on JDK 11 in docker exit with status 2 when we can't find binlog files v1.25.0 : \"mah mah mah my corona. I'm sorry. I'm sorry.\" swap un-maintained snaq.db with C3P0. support eu datadog metrics protect against lost connections during key queries (bootstrapping, heartbeats, postition setting) v1.24.2 : \"#shelterinstyle\" bugfix parsing errors: compressed columns, exchange partitions, parenthesis-enclosed default values, drop column foo.t . add partition-by-random feature. update jackson-databind to get security patch fix redis channel interpolation on RPUSH v1.24.1 : \"pixies in my head all damn week\" allow jdbc_options on secondary connections fix a crash in bootstrapping / javascript filters fix a regression in message.publish.age metric v1.24.0 : \"la la la la la la low\" add comments field to bootstrapping, thanks Tom Collins fix sql bug with #comments style comments v1.23.5 : \"And I get so stuck in my head - Lost in all the lies, nihilistic backslide\" Update bootstrap documentation Bump drop wizard metrics to support Java versions 10+ v1.23.4 : \"Try to be kinder to people who bore you, You're probably boring them too.\" Bump and override dependencies to fix security vulnerabilities. Update redis-key config options list changes v1.23.3 : \"but that's not the way it feels\" pubsubDelayMultiplier may now be 1.0 allow %{database} and %{topic} interpolation into redis producer docs updates setup default client_id in maxwell-bootstrap util v1.23.2 : \"you enjoy it every time\" upgrade jackson stop passing maxwell rows through the JS filter. too dangerous. v1.23.1 : \"the new barrista\" Add option for XADD (redis streams) operation Add configuration flag for tuning transaction buffer memory sectionalize help text v1.23.0 : \"When it breaks If it breaks We will see\" Added AWS FIFO support Add retry and batch settings to pubs producer Add support for age SLO metrics v1.22.6 : \"the things that keep your, like, dresses, like\" upgrade mysql-connector-java to 8.0.17 use a newer docker image as base list changes v1.22.5 : \"all of the names\" bugfix for bootstrapping off a split replica that doesn't contain a \"maxwell\" database Fix a parser issue with db.table.column style column names v1.22.4 : \"Last Christmans, I gave you my heart\" Add row type to fallback message Upgrade jackson-databind v1.22.3 : \"my doubt, my failings\" fix issue with google pubsub in 1.22.2 v1.22.2 : \"some girls\" fix an issue with bootstrapping-on-replicas add --output_primary_keys and --output_primary_key_columns fix a very minor memory leak with blacklists v1.22.1 : \"a snow covered field\" fix crash in rabbit-mq producer better support for maxwell + azure-mysql remove bogus different-host bootstrap check some security upgrades v1.22.0 : \"through the roof, and underground\" Bootstrapping has been reworked and is now available in all setups, including those in which the maxwell store is split from the replicator. cleanup and fix a deadlock in the kafka fallback queue logic add .partition_string = to javascript filters v1.21.1 : \"ohhhhhh oh oh\" Upgrade binlog connector. Should fix issues around deserialization errors. v1.21.0 : \"through the roof\" Bootstrapping output no longer contain binlog positions. Please update any code that relies on this. Fix 3 parser issues. v1.20.0 : \"and so you learn the only way to go is\" add support for partitioning by transaction ID thx @hexene add support for a kafka \"fallback\" topic to write to when a message fails to write add UJIS charset support parser bug: multiple strings concatenate to make one default string parser bug: deal with bizarre column renames which are then referenced in AFTER column statements v1.19.7 : \"in every corner of your room\" fix a parser error with empty sql comments interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei v1.19.6 : \"set up for you\" Further fixes for GTID-reconnection issues. Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master, thanks @acampoh v1.19.5 : \"when there is trap\" Fixes for unreliable connections wrt to GTID events; previously we restart in any old position, now we throw away the current transaction and restart the replicator again at the head of the GTID event. v1.19.4 : \"and underground\" Fixes for a maxwell database not making it through the blacklist Add output_null_zerodates parameter to control how we treat '0000-00-00' v1.19.3 : \"through the roof\" Add a universal backpressure mechanism. This should help people who were running into out-of-memory situations while bootstrapping. v1.19.2 : \"the same I wore last night\" Include schema_id in bootstrap events add more logging around binlog connector losing connection add retry logic to redis some aws fixes allow pushing JS hashes/arrays into data from js filters list changes v1.19.1 : \"the swoop here doesn't change things one bit\" Handle mysql bit literals in DEFAULT statements blacklist out CREATE ROLE etc upgrade dependencies to pick up security issues v1.19.0 : \"whole lotta milka\" mysql 8 support! utf8 enum values are supported now fix #1125, bootstrapping issue for TINYINT(1) fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\" only resume bootstraps that are targeted at this client_id fixes for blacklists and heartbeats. Did I ever mention blacklists are a terrible idea? v1.18.0 : \"hello from the Andes\" memory optimizations for large schemas (especially shareded schemas with lots of duplicates) add support for an http endpoint to support Prometheus metrics allow javascript filters to access the row query object javascript filters now run in the bootstrap process support for non-latin1 column names add --output_schema_id option better handling of packet-too-big errors from Kinesis add message.publish.age metric v1.17.1 : \"ay, ay, ay\" fix a regression around filters + bootstrapping fix a regression around filters + database-only-ddl v1.17.0 : \"monday, not sunday tuesday\" v1.17.0 brings a new level of configurability by allowing you to inject a bit of javascript into maxwell's processing. Should be useful! Also: fix regression for Alibaba RDS tables v1.16.1 : \"the 90 degree angle thing\" Fix Bootstrapping for JSON columns add --recapture_schema flag for when ya wanna start over add kafka 1.0 libraries, make them default v1.16.0 : \"kind of sort of a reference to something\" v1.16.0 brings a rewrite of Maxwell's filtering system, giving it a concise list of rules that are executed in sequence. It's now possible to exclude tables from a particular database, exclude columns matching a value, and probably some other use cases. See http://maxwells-daemon.io/config/#filtering for details. v1.15.0 : \"I'm sure I'm being supportive here.\" This is a bug-fix release, but it's big enough I'm giving it a minor version. Fix a very old bug in which DDL rows were writing the start of the row into maxwell.positions , leading to chaos in some scenarios where maxwell managed to stop on the row and double-process it, as well as to a few well-meaning patches. Fix the fact that maxwell was outputting \"next-position\" instead of \"position\" of a row into JSON. Fix the master-recovery code to store schema that corresponds to the start of a row, and points the replicator at the next-position. Much thanks to Tim, Likun and others in sorting this mess out. v1.14.7 : \"casamir pulaski day\" add RowMap#getRowQuery, thx @saimon7 revert alpine-linux docker image fiasco fix RawJSONString not serializable, thx @niuhaifeng v1.14.6 : \"gimme one sec, I need to grab something\" Fix docker image v1.14.5 : \"he looks funny, he moves funny\" reduce docker image footprint add benchmarking framework performance improvements for date/datetime columns fix parser error on UPGRADE PARTITIONING v1.14.4 : \"chinese food\" Fix race condition in SchemaCapturer v1.14.3 : \"what's for lunch?\" Enable jvm metrics v1.14.2 : \"bork bork bork\" fix regression in 1.14.1 around bootstrapping host detection fix heartbeating code around table includes v1.14.1 : \"half asleep in frog pajamas\" bootstraps can now take a client_id improved config validation for embedded mode v1.14.0 : \"cats, cats, more cats. sadness at lack of cats.\" new feature --output_xoffset to uniquely identify rows within transactions, thx Jens Gyti Bug fixes around \"0000-00-00\" times. Bug fixes around dates pre 1000 AD v1.13.5 : \"cyclone keni is real\" Support environment variable based configuration v1.13.4 : \"it was just a dream\" Added possibility to do not declare the rabbitmq exchange. v1.13.3 : \"winner winner chicken dinner\" Add logging for binlog errors Maven warning fix Do not include current position DDL schema to avoid processing DDL twice Always write null fields in primary key fields Bugfix: fix http_path_prefix command line option issue v1.13.2 : \"I just bought them to sleep in\" fix a bug with CHARACTER SET = DEFAULT maxwell now eclipse-friendly. configurable bind-address for maxwell's http server v1.13.1 : \"line up your exes in song\" redis producer now supports LPUSH, thx @m-denton RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti bugfix: fix jdbc option parsing when value contains = bugfix: apparently the SQS producer was disabled bugfix: fix a situation where adding a second client could cause schemas to become out of sync support for --daemon v1.13.0 : \"sorry, I burned your clothes\" proper SSL connection support, thanks @cadams5 support for including original SQL in insert/update/deletes, thanks @saimon7 fixes for float4, float8 and other non-mysql datatypes bump kinesis lib to 0.12.8 fix for bug when two databases share a single table v1.12.0 : \"Cold Feet, literally and metaphorically.\" Support for injecting a custom producer, thanks @tomcollinsproject New producer for Amazon SQS, thanks @vikrant2mahajan Maxwell can now filter rows based on column values, thanks @finnplay Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay DDL output can now optionally include the source SQL, thanks @sungjuly Support for double-quoted table/database/etc names rabbitmq option for persistent messages, thanks @d-babiak SQL parser bugfix for values like +1.234, thanks @hexene v1.11.0 : \"the latest, the greatest\" - default kafka client upgrades to 0.11.0.1 - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803) v1.10.9 : \"no one left behind\" We recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9. Add missing Kafka clients Listen and report on binlog connector lifecycle events for better visibility Reduce docker image size v1.10.8 : \"what doesn't kill you makes you stronger\" Fix docker builds Add Google Cloud Pub/Sub producer RabbitMQ producer enhancements v1.10.7 : \"it's never too l8!\" Java 8 upgrade Diagnostic health check endpoint Encryption Documentation update: encryption, kinesis producer, schema storage fundamentals, etc. v1.10.6 : \"a new starter is here\" Binlog-connector upgrade Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special If master recovery is enabled, Maxwell cleans up old positions for the same server and client id v1.10.5 : \"half asleep on her couch\" Shyko's binlog-connector is now the default and only replication backend available for maxwell. v1.10.4 : \"shutdown --harder\" Notable changes: Shutdown hardening. If maxwell can't shut down (because the kafka producer is in a bad state and close() never terminates, for example), it would previously stall and process no messages. Now, shutdown is run in a separate thread and there is an additional watchdog thread which forcibly kills the maxwell process if it can't shut down within 10 seconds. Initial support for running maxwell from java, rather then as its own process. This mode of operation is still experimental, but we'll accept PRs to improve it (thanks Geoff Lywood). Fix incorrect handling of negative (pre-epoch dates) when using binlog_connector mode (thanks Geoff Lywood). v1.10.3 : \"1.10.2-and-a-bit\" tiny release to fix a units error in the replication.lag metric (subtracting seconds from milliseconds) v1.10.2 : \"just in time for tomorrow\" added metrics: \"replication.queue.time\" and \"inflightmessages.count\" renamed \"time.overall\" metric to \"message.publish.time\" documentation updates (thanks Chintan Tank) v1.10.1 : \"forgive and forget\" The observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes: Support Kafka 0.10.2 Stop procesing RDS hearbeats Keep maxwell heartbeat going every 10 seconds when database is quiet Allow for empty double-quoted string literals for database schema changes Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error v1.10.0 : \"slightly more ones than zeroes\" This is a small release, primarily around a change to how schemas are stored. Maxwell now stores the last_heartbeat_read with each entry in the schemas table, making schema management more resilient to cases where binlog numbers are reused, but means that you must take care if you need to roll back to an earlier version. If you deploy v1.10.0, then roll back to an earlier version, you should manually update all schemas . last_heartbeat_read values to 0 before redeploying v1.10.0 or higher. Other minor changes: allow negative default numbers in columns only store final binlog position if it has changed blacklist internal aurora table `rds_heartbeat*' log4j version bump (allows for one entry per line JSON logging) v1.9.0 : \"now with added whimsy\" Maxwell 1.9 adds one main feature: monitoring support, contributed by Scott Ferguson. Multiple backends can be configured, read the updated docs for full details. There's also some bugfixes: filter DDL messages based on config determine newest schema from binlog order, not creation order add task manager to shutdown cleanly on error minor logging improvements v1.8.2 : \"just as the postcards wept\" Bugfix release. maxwell would crash on a quoted partition name fixes for alters on non-string tables containing VARCHAR use seconds instead of milliseconds for DDL messages v1.8.1 : \"famous is faster, don't have to be talented\" performance improves in capturing and restoring schema, thx Joren Minnaert Allow for capturing from a separate mysql host (adds support for using Maxscale as a replication proxy), thx Adam Szkoda v1.8.0 : \"upbeat, honest, contradictory\" In version 1.8.0 Maxwell gains alpha support for GTID-based positions! All praise due to Henry Cai. v1.7.2 : \"comparing self to better\" Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a binlog event, leading to crashes or in some cases data mismatches. v1.7.1 : \"blame it on your seratonin\" bootstrapping now can take a --where clause performance improvements in the kafka producer v1.7.0 : \"lucky me, lucky mud\" Maxwell 1.7 brings 2 major new, alpha features. The first is Mysql 5.7 support, including JSON column type support and handling of 5.7 SQL, but not including GTID support yet. This is based on porting Maxwell to Stanley Shyko's binlog-connector library. Thanks to Stanley for his amazing support doing this port. The second major new feature is a producer for Amazon's Kinesis streams, This was contributed in full by the dogged and persistent Thomas Dziedzic. Check it out with --producer=kinesis . There's also some bugfixes: - Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson - allow CHECK() statements inside column definitions v1.6.0 : \"give me a quest\" This is mostly a bugfix release, but it gets a minor version bump due to a single change of behavior: dates and timestamps which mysql may accept, but are considered invalid (0000-00-00 is a notable example) previously had inconsistent behavior. Now we convert these to NULL. Other bugfixes: - heartbeats have moved into their own table - more fixes around alibaba rds - ignore DELETE statements that are output for MEMORY tables upon server restart - allow pointing maxwell to a pre-existing database v1.5.2 : \"french banana\" add support for kafka 0.10.1 @ smferguson master recovery: cleanup positions from previous master; prevent errors on flip-back. fix a bug that would trigger in certain cases when dropping a column that was part of the primary-key v1.5.1 : \"1.5.1 is just 1.5.1\" This is a bugfix release. - fixes for bootstrapping with an alternative maxwell-schema name and an include_database filter, thanks Lucian Jones - fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson - ignore the RDS table mysql.ha_health_check table - Get the bootstrapping process to output NULL values. - fix a quoting issue in the bootstrap code, thanks @mylesjao. v1.5.0 : \"someone, somewhere, is still smoking cigarettes, damnit\" CHANGE: Kafka producer no longer ships with hard-coded defaults. Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\" configured to your liking. bugfix: fix a regression in handling ALTER TABLE change c int after b statements warn on servers with missing server_id v1.4.2 : \"drawer cat is back\" kafka 0.10.0 support, as well as a re-working of the --kafka_version command line option. v1.4.1 : \"cat snores\" support per-table topics, Thanks @smferguson and @sschatts. fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson v1.4.0 : \"deep, insomniac character flaws\" 1.4.0 brings us two nice new features: - partition-by-column: see --kafka_partition_columns. Thanks @smferguson - output schema changes as JSON: see --output_ddl. Thanks @xmlking - As well as a fix around race conditions on shutdown. v1.3.0 : \"yogg-saron\" support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan support for outputting server_id & thread_id, thanks @sagiba fix a race condition in bootstrap support v1.2.2 : \"bats wearing frog pajamas\" Maxwell will now include by default fields with NULL values (as null fields). To disable this and restore the old functionality where fields were omitted, pass --output_nulls=false Fix an issue with multi-client support where two replicators would ping-pong heartbeats at each other Fix an issue where a client would attempt to recover a position from a mismatched client_id Fix a bug when using CHANGE COLUMN on a primary key v1.2.1 : \"point-ones are a sad and inevitable fact\" This is a bugfix release. - fix a parser bug around ALTER TABLE CHARACTER SET - fix bin/maxwell to pull in the proper version of the kafka-clients library v1.2.0 : \"just here, not to talk to you\" 1.2.0 is a major release of Maxwell that introduces master recovery features; when a slave is promoted to master, Maxwell is now capable of recovering the position. See the --master_recovery flag for more details. It also upgrades the kafka producer library to 0.9. If you're using maxwell with a kafka 0.8 server, you must now pass the --kafka0.8 flag to maxwell. v1.1.6 : \"pithy\" minor bugfix in which maxwell with --replay mode was trying to write heartbeats v1.1.5 : \"my brain is a polluted mess\" @dadah89 adds --output_binlog_position to optionally output the position with the row @dadah89 adds --output_commit_info to turn off xid/commit fields maxwell now supports tables with partitions maxwell now supports N maxwells per-server. see the client_id / replica_server_id options. two parser fixes, for engine= innodb and CHARSET ASCII lay the ground work for doing master recovery; we add a heartbeat into the positions table that we can co-ordinate around. v1.1.4 : \"george flunk\" add support for a bunch more charsets (gbk, big5, notably) fix Maxwell's handling of kafka errors - previously we were trying to crash Maxwell by throwing a RuntimeException out of the Kafka Producer, but this was a failure. Now we log and skip all errors. v1.1.3 : \"the button I push to not have to go out\" This is a bugfix release, which fixes: - https://github.com/zendesk/maxwell/issues/376, a problem parsing RENAME INDEX - https://github.com/zendesk/maxwell/issues/371, a problem with the SERIAL datatype - https://github.com/zendesk/maxwell/issues/362, we now preserve the original casing of columns - https://github.com/zendesk/maxwell/issues/373, we were incorrectly expecting heartbeats to work under 5.1 v1.1.2 : \"scribbled notes on red pages\" pick up latest mysql-connector-j, fixes #369 fix an issue where maxwell could skip ahead positions if a leader failed. rework buffering code to be much kinder to the GC and JVM heap in case of very large transactions / rows inside transactions kinder, gentler help text when you specify an option incorrectly v1.1.1 : scribbled notes on blue pages fixes a race condition setting the binlog position that would get maxwell stuck v1.1.0 : \"sleep away the afternoon\" much more efficient processing of schema updates storage, especially when dealing with large schemas. @lileeyao added --exclude-columns and the --jdbc_options features @lileeyao added --jdbc_options can now blacklist entire databases new kafka key format available, using a JSON array instead of an object bugfix: unsigned integer columns were captured incorrectly. 1.1 will recapture the schema and attempt to correct the error. v1.1.0-pre4 : \"buck buck buck buck buck buck-AH!\" Eddie McLean gives some helpful patches around bootstrapping Bugfixes for the patch-up-the-schema code around unsigned ints v1.1.0-pre3 : forgot to include some updates that back-patch unsigned column problems v1.1.0-pre2 : \"yawn yawn\" fix performance issues when capturing schema in AWS Aurora fix a bug in capturing unsigned integer columns v1.0.1 : \"bag of oversized daisies\" fixes a parsing bug with CURRENT_TIMESTAMP() v1.0.0 : \"Maxwell learns to speak\" Since v0.17.0, Maxwell has gotten: - bootstrapping support - blacklisting for tables - flexible kafka partitioning - replication heartbeats - GEOMETRY columns - a whole lotta lotta bugfixes and I, Osheroff, think the damn thing is stable enough for a 1.0. So there. v1.0.0-RC3 : \"C'mon and take it\" pull in support for replication heartbeats. helps in the flakier network environs. v1.0.0-RC2 : \"same thing, just without the v\" fixes the way ALTER DATABASE charset= was handled adds proper handling of ALTER TABLE CONVERT TO CHARSET v1.0.0-RC1 : \"Richard Buckner's release\" modifications to the way the bootstrap utility works fix a race condition crash bug in bootstrapping fix a parser bug v1.0.0-PRE2 : \"an embarassment of riches\" 1.0.0-PRE2 brings in a lot of changes that got merged while we were testing out PRE1. so, hey. - Configurable names for the maxwell schema database (Kristian Kaufman) - Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman) - Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman) - support GEOMETRY columns, output as well-known-text - add --blacklist_tables option to fully ignore excessive schema changes (Nicolas Maquet) - bootstrap rows now have 'bootstrap-insert' type v1.0.0-PRE1 : \"drunk conversations with sober people\" Here we have the preview release of @nmaquet's excellent work around bootstrapping initial versions of mysql tables. v0.17.0 : \"wrists of William\" v0.17 is a large bugfix release with one new feature. - FEATURE: allow specifying an alternative mysql schema-storage server and replication server - BUGFIX: properly handle case-sensitivity by aping the behavior of the master server. Fixes #230. - BUGFIX: parse some forms of CHECK( ... ) statements. Fixes #203. - BUGFIX: many more SQL-parser fixes. We are mostly through some thousands of lines of SQL produced by mysql-test. v0.16.2 : \"The best laid plans\" This is a large-ish bugfix release. - Support, with reservations, binlog_row_image=MINIMAL - parser bug: handle table names that look like floating points - parser bug: fix for entity names that have '.', '\\', etc in them - handle UPPERCASE encoding names - support UCS2 (start trying to operate ok on the mysql-test suite) - use ObjectOutputStream.reset to fix memory leaks when buffering to disk v0.16.1 : \"me and room service\" This is a bug-fix-roundup release: - support ALTER DATABASE - fix a bunch of parse errors: we've started running mysql-test at maxwell and are fixing up failures. - some modifications to the overflow-to-disk logic; we buffer the input and output, and we fix a memory leak v0.16.0 : \"Kristian Kaufmann's version\" Version 0.16.0 introduces a feature where UPDATE statements will now show both the new row image and the old values of the fields that changed. Thanks @kristiankaufmann v0.15.0 : \"the littlest little city\" fix a parse problem with indices ordered by ASC/DESC v0.15.0-RC1 : \"it's later than you think\" large transactions now buffer to disk instead of crushing maxwell. support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters v0.14.6 : \"It's about being American. Sort of.\" fix TIME column support fix parsing on millisecond precision column defintions fix CREATE SCHEMA parsing v0.14.5 : \"false is the new true\" handle BOOLEAN columns with true/false defaults v0.14.4 : \"You'd think we'd be at 1.0 by now, wouldn't you?\" fixes parsing of \"mysql comments\" ( /*! .. */ ) More performance improvements, another 10% in a tight loop. v0.14.3 : \"Peanuts. My girlfriend thinks about peanuts.\" fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema. v0.14.2 : \"Maxwell Sandvik 88\" capture the mysql database along with the rest of the schema. Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. v0.14.1 : \"be liberal in what you accept. Even if nonsensical.\" fixes a parser bug around named PRIMARY KEYs. v0.14.0 : \"the slow but inevitable slide\" This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions. v0.13.1 : \"well that was somewhat expected\" v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. v0.13.0 contains: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. v0.13.0 : \"Malkovich Malkovich Malkovich Sheldon?\" Lucky release number 13 brings some reasonably big changes: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. This release has a pretty bad bug. do not use. v0.12.0 : \"what do I call them? Slippers? Why, are you jealous?\" add support for BIT columns. v0.11.4 : \"13 steps\" this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event. I really need to fix this at a lower level, ie the open-replicator level. v0.11.3 : \".. and the other half is to take the bugs out\" this is a bugfix release: - fix problems with table creation options inside alter statements ( ALTER TABLE foo auto_increment=10 ) - fix a host of shutdown-procedure bugs the test suite should also be way more reliable, not like you care. v0.11.2 : \"savage acts of unprovoked violence are bad\" This is a bugfix release. It includes: - soft deletions of maxwell.schemas to fix A->B->A master swapping without creating intense replication delay - detect and fail early if we see binlog_row_image=minimal - kill off maxwell if the position thread dies - fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!) v0.11.1 : \"dog snoring loudly\" maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid. this should prevent the worst of cases. v0.11.0 : \"cat waving gently\" This release of Maxwell preserves transaction information in the kafka stream by adding a xid key in the JSON object, as well as a commit key for the final row inside the transaction. It also contains a bugfix around server_id handling. v0.10.1 : \"all 64 of your bases belong to... shut up, internet parrot.\" proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded) fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash make table option parsing more lenient v0.11.0-RC1 : \"goin' faster than a rollercoaster\" merge master fixes v0.10.0 : \"The first word is French\" Mysql 5.6 checksum support! some more bugfixes with the SQL parser v0.11.0-PRE4 : \"except for that other thing\" bugfix on v0.11.0-PRE3 v0.11.0-PRE3 : \"nothing like a good night's sleep\" handle SAVEPOINT within transactions downgrade unhandled SQL to a warning v0.11.0-PRE2 : \"you really need to name a PRE release something cutesy?\" fixes for myISAM \"transactions\" v0.11.0-PRE1 : \"A slow traffic jam towards the void\" fix a server_id bug (was always 1 in maxwell.schemas) JSON output now includes transaction IDs v0.10.0-RC4 : \"Inspiring confidence\" deal with BINARY flag in string column creation. v0.9.5 : \"Long story short, that's why I'm late\" handle the BINARY flag in column creation v0.10.0-RC3 : \"Except for that one thing\" handle \"TRUNCATE [TABLE_NAME]\" statements v0.10.0-RC2 : \"RC2 is always a good sign.\" fixes a bug with checksum processing. v0.10.0-RC1 : \"verify all the things\" upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell. v0.9.4 : \"we've been here before\" allow a configurable number (including unlimited) of schemas to be stored v0.9.3 : \"some days it's just better to stay in bed\" bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes v0.9.2 : \"Cat's tongue\" bump open-replicator buffer to 50mb by default log to STDERR, not STDOUT --output_file option for file producer v0.9.1 : \"bugs, bugs, bugs, lies, statistics\" Maxwell is now aware that column names are case-insenstive fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master. v0.9.0 : Vanchi says \"eat\" Also, vanchi is so paranoid he's worried immediately about this. mysql 5.6 support (without checksum support, yet) fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE) v0.8.1 : \"Pascal says Bonjour\" minor bugfix release around mysql connections going away. v0.8.0 : the cat never shuts up add \"ts\" field to row output add --config option for passing a different config file support int1, int2, int4, int8 columns v0.7.2 : \"all the sql ladies\" handle inline sql comments ignore more user management SQL v0.7.1 : \"not hoarders\" only keep 5 most recent schemas v0.7.0 : 0.7.0, \"alameda\" handle CURRENT_TIMESTAMP parsing properly better binlog position sync behavior v0.6.3 : 0.6.3 better blacklist for CREATE TRIGGER v0.6.2 : v0.6.2 maxwell now ignores SAVEPOINT statements. v0.6.1 : v0.6.1 fixes a bug with parsing length-limited indexes. v0.6.0 : kafkakafkakafa Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy. It also fixes a couple of bugs in the query parsing path. v0.5.0 : 0.5.0 -- \"People who put commas in column names deserve undefined behavior\" maxwell now captures primary keys on tables. We'll use this to form kafka key names later. maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order. v0.4.0 : 0.4.0, \"unboxed cat\" v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support. v0.3.0 : 0.3.0 This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. It also enables kafka gzip compression by default. v0.2.2 : 0.2.2 Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. v0.2.1 : v0.2.1 version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others. v0.2.0 : 0.2.0 This release gets Maxwell storing the last-written binlog position inside the mysql master itself. v0.1.4 : 0.1.4 support --position_file param v0.1.3 : 0.1.3 Adds kafka command line options. v0.1.1 : 0.1.1 v0.1.1, a small bugfix release. v0.1 : 0.1 This is the first possible release of Maxwell that might work. It includes some exceedingly basic kafka support, and JSON output of binlog deltas.","title":"Changelog"},{"location":"changelog/#maxwell-changelog","text":"","title":"Maxwell changelog"},{"location":"changelog/#v1310-84-tent-cabin","text":"Add producer for NATS streaming server","title":"v1.31.0: \"84 tent cabin\""},{"location":"changelog/#v1300-all-of-this-has-happened-before","text":"support server-sent heartbeating on the binlog connection via --binlog-heartbeat can connect to rabbitmq by URL, supports SSL connections fix parser bug with multiline SQL target JDK 11 -- we have dropped support for JDK 8 ability to send a microsecond timestamp via --output_push_timestamp fixes for odd azure mysql connection failures","title":"v1.30.0: \"all of this has happened before\""},{"location":"changelog/#v1292-i-now-know-the-meaning-of-shame","text":"fix for terrible performance regression in bootstrapping","title":"v1.29.2: \"i now know the meaning of shame\""},{"location":"changelog/#v1291-depluralize","text":"small bugfix release, fixes binlog event type processing in mysql 8","title":"v1.29.1: \"depluralize\""},{"location":"changelog/#v1290-i-dont-know-i-dont-know-i-dont-know","text":"High Availability support via jgroups-raft rework --help text","title":"v1.29.0: \"i don't know, i don't know, i don't know\""},{"location":"changelog/#v1282-fantasy-baseball","text":"fix for encryption parsing error on table creation some logging around memory usage in RowMapBuffer","title":"v1.28.2: \"fantasy baseball\""},{"location":"changelog/#v1281-bootras-bootras-gallliiiii","text":"fix http server issue in 1.28.0","title":"v1.28.1: \"bootras bootras gallliiiii\""},{"location":"changelog/#v1280-stardew-mania","text":"schema compaction! with the new --max_schemas option, maxwell will periodically roll up the maxwell . schemas table, preventing it from growing infinitely long. fix metricsAgeSloMS calculation support SRID columns fix parsing of complex INDEX(CAST()) statements various dependency bumps","title":"v1.28.0: \"stardew mania\""},{"location":"changelog/#v1271-red-bag-red-bag","text":"redis producer gets sentinal support fix a double-reconnect race condition file producer honors javascript row-suppression better error messaging when we lack REPLICATION SLAVE privs miscellaneous dependency bumps","title":"v1.27.1: \"red bag?  red bag\""},{"location":"changelog/#v1270-running-water","text":"better support for empty/null passwords allow bootstrap utility to query replication_host a few library upgrades, notably pubsub and kinesis library bootstrap connection uses jdbc_options properly add logging for when we hit out of sync schema exceptions allow for partitioning by thread_id, thx @gogov fresh and clean documentation","title":"v1.27.0: \"running water\""},{"location":"changelog/#v1264-no-songs-here","text":"support now() function with precision","title":"v1.26.4: \"No songs here\""},{"location":"changelog/#v1263-the-worst-song-in-the-goddamn-world","text":"use pooled redis connections, fixes corruption when redis was accessed from multiple threads (bootstrap/producer), thanks @lucastex fix date handling of '0000-01-01' fix race condition in binlog reconnect logic","title":"v1.26.3: \"the worst song in the goddamn world\""},{"location":"changelog/#v1262-dave-the-butcher","text":"bootstraps can be scheduled in the future by setting the started_at column, thanks @lucastex two mysql 8 fixes; one for a DEFAULT(function()) parse error, one for supporting DEFAULT ENCRYPTION","title":"v1.26.2: \"dave the butcher\""},{"location":"changelog/#v1261-maybe-we-can-break-your-ankle-clean-and-unsuspiciously","text":"fixes for redis re-connection login, thanks much @lucastex","title":"v1.26.1: \"maybe we can break your ankle / clean and unsuspiciously\""},{"location":"changelog/#v1260-tip-the-waitress-feed-her-cocaine-habit","text":"We now support mysql 8's caching_sha2_password authentication scheme support for converting JSON field names to camelCase","title":"v1.26.0: \"tip the waitress, feed her cocaine habit\""},{"location":"changelog/#v1253-bye-bolinas","text":"fixes memory leak in mysql-binlog-connector fixes exceptions that occur when a connection passes wait_timeout","title":"v1.25.3: \"bye, bolinas\""},{"location":"changelog/#v1252-love-potion-9","text":"Fixes for a long standing JSON bug in 8.0.19+","title":"v1.25.2: \"love potion #9\""},{"location":"changelog/#v1251-nowhere-to-put-it","text":"issue #1457, ALTER DATABASE with implicit database name maxwell now runs on JDK 11 in docker exit with status 2 when we can't find binlog files","title":"v1.25.1: \"nowhere to put it\""},{"location":"changelog/#v1250-mah-mah-mah-my-corona-im-sorry-im-sorry","text":"swap un-maintained snaq.db with C3P0. support eu datadog metrics protect against lost connections during key queries (bootstrapping, heartbeats, postition setting)","title":"v1.25.0: \"mah mah mah my corona.  I'm sorry.  I'm sorry.\""},{"location":"changelog/#v1242-shelterinstyle","text":"bugfix parsing errors: compressed columns, exchange partitions, parenthesis-enclosed default values, drop column foo.t . add partition-by-random feature. update jackson-databind to get security patch fix redis channel interpolation on RPUSH","title":"v1.24.2: \"#shelterinstyle\""},{"location":"changelog/#v1241-pixies-in-my-head-all-damn-week","text":"allow jdbc_options on secondary connections fix a crash in bootstrapping / javascript filters fix a regression in message.publish.age metric","title":"v1.24.1: \"pixies in my head all damn week\""},{"location":"changelog/#v1240-la-la-la-la-la-la-low","text":"add comments field to bootstrapping, thanks Tom Collins fix sql bug with #comments style comments","title":"v1.24.0: \"la la la la la la low\""},{"location":"changelog/#v1235-and-i-get-so-stuck-in-my-head-lost-in-all-the-lies-nihilistic-backslide","text":"Update bootstrap documentation Bump drop wizard metrics to support Java versions 10+","title":"v1.23.5: \"And I get so stuck in my head - Lost in all the lies, nihilistic backslide\""},{"location":"changelog/#v1234-try-to-be-kinder-to-people-who-bore-you-youre-probably-boring-them-too","text":"Bump and override dependencies to fix security vulnerabilities. Update redis-key config options list changes","title":"v1.23.4: \"Try to be kinder to people who bore you, You're probably boring them too.\""},{"location":"changelog/#v1233-but-thats-not-the-way-it-feels","text":"pubsubDelayMultiplier may now be 1.0 allow %{database} and %{topic} interpolation into redis producer docs updates setup default client_id in maxwell-bootstrap util","title":"v1.23.3: \"but that's not the way it feels\""},{"location":"changelog/#v1232-you-enjoy-it-every-time","text":"upgrade jackson stop passing maxwell rows through the JS filter. too dangerous.","title":"v1.23.2: \"you enjoy it every time\""},{"location":"changelog/#v1231-the-new-barrista","text":"Add option for XADD (redis streams) operation Add configuration flag for tuning transaction buffer memory sectionalize help text","title":"v1.23.1: \"the new barrista\""},{"location":"changelog/#v1230-when-it-breaks-if-it-breaks-we-will-see","text":"Added AWS FIFO support Add retry and batch settings to pubs producer Add support for age SLO metrics","title":"v1.23.0: \"When it breaks If it breaks We will see\""},{"location":"changelog/#v1226-the-things-that-keep-your-like-dresses-like","text":"upgrade mysql-connector-java to 8.0.17 use a newer docker image as base list changes","title":"v1.22.6: \"the things that keep your, like, dresses, like\""},{"location":"changelog/#v1225-all-of-the-names","text":"bugfix for bootstrapping off a split replica that doesn't contain a \"maxwell\" database Fix a parser issue with db.table.column style column names","title":"v1.22.5: \"all of the names\""},{"location":"changelog/#v1224-last-christmans-i-gave-you-my-heart","text":"Add row type to fallback message Upgrade jackson-databind","title":"v1.22.4: \"Last Christmans, I gave you my heart\""},{"location":"changelog/#v1223-my-doubt-my-failings","text":"fix issue with google pubsub in 1.22.2","title":"v1.22.3: \"my doubt, my failings\""},{"location":"changelog/#v1222-some-girls","text":"fix an issue with bootstrapping-on-replicas add --output_primary_keys and --output_primary_key_columns fix a very minor memory leak with blacklists","title":"v1.22.2: \"some girls\""},{"location":"changelog/#v1221-a-snow-covered-field","text":"fix crash in rabbit-mq producer better support for maxwell + azure-mysql remove bogus different-host bootstrap check some security upgrades","title":"v1.22.1: \"a snow covered field\""},{"location":"changelog/#v1220-through-the-roof-and-underground","text":"Bootstrapping has been reworked and is now available in all setups, including those in which the maxwell store is split from the replicator. cleanup and fix a deadlock in the kafka fallback queue logic add .partition_string = to javascript filters","title":"v1.22.0: \"through the roof, and underground\""},{"location":"changelog/#v1211-ohhhhhh-oh-oh","text":"Upgrade binlog connector. Should fix issues around deserialization errors.","title":"v1.21.1: \"ohhhhhh oh oh\""},{"location":"changelog/#v1210-through-the-roof","text":"Bootstrapping output no longer contain binlog positions. Please update any code that relies on this. Fix 3 parser issues.","title":"v1.21.0: \"through the roof\""},{"location":"changelog/#v1200-and-so-you-learn-the-only-way-to-go-is","text":"add support for partitioning by transaction ID thx @hexene add support for a kafka \"fallback\" topic to write to when a message fails to write add UJIS charset support parser bug: multiple strings concatenate to make one default string parser bug: deal with bizarre column renames which are then referenced in AFTER column statements","title":"v1.20.0: \"and so you learn the only way to go is\""},{"location":"changelog/#v1197-in-every-corner-of-your-room","text":"fix a parser error with empty sql comments interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei","title":"v1.19.7: \"in every corner of your room\""},{"location":"changelog/#v1196-set-up-for-you","text":"Further fixes for GTID-reconnection issues. Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master, thanks @acampoh","title":"v1.19.6: \"set up for you\""},{"location":"changelog/#v1195-when-there-is-trap","text":"Fixes for unreliable connections wrt to GTID events; previously we restart in any old position, now we throw away the current transaction and restart the replicator again at the head of the GTID event.","title":"v1.19.5: \"when there is trap\""},{"location":"changelog/#v1194-and-underground","text":"Fixes for a maxwell database not making it through the blacklist Add output_null_zerodates parameter to control how we treat '0000-00-00'","title":"v1.19.4: \"and underground\""},{"location":"changelog/#v1193-through-the-roof","text":"Add a universal backpressure mechanism. This should help people who were running into out-of-memory situations while bootstrapping.","title":"v1.19.3: \"through the roof\""},{"location":"changelog/#v1192-the-same-i-wore-last-night","text":"Include schema_id in bootstrap events add more logging around binlog connector losing connection add retry logic to redis some aws fixes allow pushing JS hashes/arrays into data from js filters list changes","title":"v1.19.2: \"the same I wore last night\""},{"location":"changelog/#v1191-the-swoop-here-doesnt-change-things-one-bit","text":"Handle mysql bit literals in DEFAULT statements blacklist out CREATE ROLE etc upgrade dependencies to pick up security issues","title":"v1.19.1: \"the swoop here doesn't change things one bit\""},{"location":"changelog/#v1190-whole-lotta-milka","text":"mysql 8 support! utf8 enum values are supported now fix #1125, bootstrapping issue for TINYINT(1) fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\" only resume bootstraps that are targeted at this client_id fixes for blacklists and heartbeats. Did I ever mention blacklists are a terrible idea?","title":"v1.19.0: \"whole lotta milka\""},{"location":"changelog/#v1180-hello-from-the-andes","text":"memory optimizations for large schemas (especially shareded schemas with lots of duplicates) add support for an http endpoint to support Prometheus metrics allow javascript filters to access the row query object javascript filters now run in the bootstrap process support for non-latin1 column names add --output_schema_id option better handling of packet-too-big errors from Kinesis add message.publish.age metric","title":"v1.18.0: \"hello from the Andes\""},{"location":"changelog/#v1171-ay-ay-ay","text":"fix a regression around filters + bootstrapping fix a regression around filters + database-only-ddl","title":"v1.17.1: \"ay, ay, ay\""},{"location":"changelog/#v1170-monday-not-sunday-tuesday","text":"v1.17.0 brings a new level of configurability by allowing you to inject a bit of javascript into maxwell's processing. Should be useful! Also: fix regression for Alibaba RDS tables","title":"v1.17.0: \"monday, not sunday tuesday\""},{"location":"changelog/#v1161-the-90-degree-angle-thing","text":"Fix Bootstrapping for JSON columns add --recapture_schema flag for when ya wanna start over add kafka 1.0 libraries, make them default","title":"v1.16.1: \"the 90 degree angle thing\""},{"location":"changelog/#v1160-kind-of-sort-of-a-reference-to-something","text":"v1.16.0 brings a rewrite of Maxwell's filtering system, giving it a concise list of rules that are executed in sequence. It's now possible to exclude tables from a particular database, exclude columns matching a value, and probably some other use cases. See http://maxwells-daemon.io/config/#filtering for details.","title":"v1.16.0: \"kind of sort of a reference to something\""},{"location":"changelog/#v1150-im-sure-im-being-supportive-here","text":"This is a bug-fix release, but it's big enough I'm giving it a minor version. Fix a very old bug in which DDL rows were writing the start of the row into maxwell.positions , leading to chaos in some scenarios where maxwell managed to stop on the row and double-process it, as well as to a few well-meaning patches. Fix the fact that maxwell was outputting \"next-position\" instead of \"position\" of a row into JSON. Fix the master-recovery code to store schema that corresponds to the start of a row, and points the replicator at the next-position. Much thanks to Tim, Likun and others in sorting this mess out.","title":"v1.15.0: \"I'm sure I'm being supportive here.\""},{"location":"changelog/#v1147-casamir-pulaski-day","text":"add RowMap#getRowQuery, thx @saimon7 revert alpine-linux docker image fiasco fix RawJSONString not serializable, thx @niuhaifeng","title":"v1.14.7: \"casamir pulaski day\""},{"location":"changelog/#v1146-gimme-one-sec-i-need-to-grab-something","text":"Fix docker image","title":"v1.14.6: \"gimme one sec, I need to grab something\""},{"location":"changelog/#v1145-he-looks-funny-he-moves-funny","text":"reduce docker image footprint add benchmarking framework performance improvements for date/datetime columns fix parser error on UPGRADE PARTITIONING","title":"v1.14.5: \"he looks funny, he moves funny\""},{"location":"changelog/#v1144-chinese-food","text":"Fix race condition in SchemaCapturer","title":"v1.14.4: \"chinese food\""},{"location":"changelog/#v1143-whats-for-lunch","text":"Enable jvm metrics","title":"v1.14.3: \"what's for lunch?\""},{"location":"changelog/#v1142-bork-bork-bork","text":"fix regression in 1.14.1 around bootstrapping host detection fix heartbeating code around table includes","title":"v1.14.2: \"bork bork bork\""},{"location":"changelog/#v1141-half-asleep-in-frog-pajamas","text":"bootstraps can now take a client_id improved config validation for embedded mode","title":"v1.14.1: \"half asleep in frog pajamas\""},{"location":"changelog/#v1140-cats-cats-more-cats-sadness-at-lack-of-cats","text":"new feature --output_xoffset to uniquely identify rows within transactions, thx Jens Gyti Bug fixes around \"0000-00-00\" times. Bug fixes around dates pre 1000 AD","title":"v1.14.0: \"cats, cats, more cats.  sadness at lack of cats.\""},{"location":"changelog/#v1135-cyclone-keni-is-real","text":"Support environment variable based configuration","title":"v1.13.5: \"cyclone keni is real\""},{"location":"changelog/#v1134-it-was-just-a-dream","text":"Added possibility to do not declare the rabbitmq exchange.","title":"v1.13.4: \"it was just a dream\""},{"location":"changelog/#v1133-winner-winner-chicken-dinner","text":"Add logging for binlog errors Maven warning fix Do not include current position DDL schema to avoid processing DDL twice Always write null fields in primary key fields Bugfix: fix http_path_prefix command line option issue","title":"v1.13.3: \"winner winner chicken dinner\""},{"location":"changelog/#v1132-i-just-bought-them-to-sleep-in","text":"fix a bug with CHARACTER SET = DEFAULT maxwell now eclipse-friendly. configurable bind-address for maxwell's http server","title":"v1.13.2: \"I just bought them to sleep in\""},{"location":"changelog/#v1131-line-up-your-exes-in-song","text":"redis producer now supports LPUSH, thx @m-denton RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti bugfix: fix jdbc option parsing when value contains = bugfix: apparently the SQS producer was disabled bugfix: fix a situation where adding a second client could cause schemas to become out of sync support for --daemon","title":"v1.13.1: \"line up your exes in song\""},{"location":"changelog/#v1130-sorry-i-burned-your-clothes","text":"proper SSL connection support, thanks @cadams5 support for including original SQL in insert/update/deletes, thanks @saimon7 fixes for float4, float8 and other non-mysql datatypes bump kinesis lib to 0.12.8 fix for bug when two databases share a single table","title":"v1.13.0: \"sorry, I burned your clothes\""},{"location":"changelog/#v1120-cold-feet-literally-and-metaphorically","text":"Support for injecting a custom producer, thanks @tomcollinsproject New producer for Amazon SQS, thanks @vikrant2mahajan Maxwell can now filter rows based on column values, thanks @finnplay Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay DDL output can now optionally include the source SQL, thanks @sungjuly Support for double-quoted table/database/etc names rabbitmq option for persistent messages, thanks @d-babiak SQL parser bugfix for values like +1.234, thanks @hexene","title":"v1.12.0: \"Cold Feet, literally and metaphorically.\""},{"location":"changelog/#v1110-the-latest-the-greatest","text":"- default kafka client upgrades to 0.11.0.1 - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803)","title":"v1.11.0: \"the latest, the greatest\""},{"location":"changelog/#v1109-no-one-left-behind","text":"We recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9. Add missing Kafka clients Listen and report on binlog connector lifecycle events for better visibility Reduce docker image size","title":"v1.10.9: \"no one left behind\""},{"location":"changelog/#v1108-what-doesnt-kill-you-makes-you-stronger","text":"Fix docker builds Add Google Cloud Pub/Sub producer RabbitMQ producer enhancements","title":"v1.10.8: \"what doesn't kill you makes you stronger\""},{"location":"changelog/#v1107-its-never-too-l8","text":"Java 8 upgrade Diagnostic health check endpoint Encryption Documentation update: encryption, kinesis producer, schema storage fundamentals, etc.","title":"v1.10.7: \"it's never too l8!\""},{"location":"changelog/#v1106-a-new-starter-is-here","text":"Binlog-connector upgrade Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special If master recovery is enabled, Maxwell cleans up old positions for the same server and client id","title":"v1.10.6: \"a new starter is here\""},{"location":"changelog/#v1105-half-asleep-on-her-couch","text":"Shyko's binlog-connector is now the default and only replication backend available for maxwell.","title":"v1.10.5: \"half asleep on her couch\""},{"location":"changelog/#v1104-shutdown-harder","text":"Notable changes: Shutdown hardening. If maxwell can't shut down (because the kafka producer is in a bad state and close() never terminates, for example), it would previously stall and process no messages. Now, shutdown is run in a separate thread and there is an additional watchdog thread which forcibly kills the maxwell process if it can't shut down within 10 seconds. Initial support for running maxwell from java, rather then as its own process. This mode of operation is still experimental, but we'll accept PRs to improve it (thanks Geoff Lywood). Fix incorrect handling of negative (pre-epoch dates) when using binlog_connector mode (thanks Geoff Lywood).","title":"v1.10.4: \"shutdown --harder\""},{"location":"changelog/#v1103-1102-and-a-bit","text":"tiny release to fix a units error in the replication.lag metric (subtracting seconds from milliseconds)","title":"v1.10.3: \"1.10.2-and-a-bit\""},{"location":"changelog/#v1102-just-in-time-for-tomorrow","text":"added metrics: \"replication.queue.time\" and \"inflightmessages.count\" renamed \"time.overall\" metric to \"message.publish.time\" documentation updates (thanks Chintan Tank)","title":"v1.10.2: \"just in time for tomorrow\""},{"location":"changelog/#v1101-forgive-and-forget","text":"The observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes: Support Kafka 0.10.2 Stop procesing RDS hearbeats Keep maxwell heartbeat going every 10 seconds when database is quiet Allow for empty double-quoted string literals for database schema changes Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error","title":"v1.10.1: \"forgive and forget\""},{"location":"changelog/#v1100-slightly-more-ones-than-zeroes","text":"This is a small release, primarily around a change to how schemas are stored. Maxwell now stores the last_heartbeat_read with each entry in the schemas table, making schema management more resilient to cases where binlog numbers are reused, but means that you must take care if you need to roll back to an earlier version. If you deploy v1.10.0, then roll back to an earlier version, you should manually update all schemas . last_heartbeat_read values to 0 before redeploying v1.10.0 or higher. Other minor changes: allow negative default numbers in columns only store final binlog position if it has changed blacklist internal aurora table `rds_heartbeat*' log4j version bump (allows for one entry per line JSON logging)","title":"v1.10.0: \"slightly more ones than zeroes\""},{"location":"changelog/#v190-now-with-added-whimsy","text":"Maxwell 1.9 adds one main feature: monitoring support, contributed by Scott Ferguson. Multiple backends can be configured, read the updated docs for full details. There's also some bugfixes: filter DDL messages based on config determine newest schema from binlog order, not creation order add task manager to shutdown cleanly on error minor logging improvements","title":"v1.9.0: \"now with added whimsy\""},{"location":"changelog/#v182-just-as-the-postcards-wept","text":"Bugfix release. maxwell would crash on a quoted partition name fixes for alters on non-string tables containing VARCHAR use seconds instead of milliseconds for DDL messages","title":"v1.8.2: \"just as the postcards wept\""},{"location":"changelog/#v181-famous-is-faster-dont-have-to-be-talented","text":"performance improves in capturing and restoring schema, thx Joren Minnaert Allow for capturing from a separate mysql host (adds support for using Maxscale as a replication proxy), thx Adam Szkoda","title":"v1.8.1: \"famous is faster, don't have to be talented\""},{"location":"changelog/#v180-upbeat-honest-contradictory","text":"In version 1.8.0 Maxwell gains alpha support for GTID-based positions! All praise due to Henry Cai.","title":"v1.8.0: \"upbeat, honest, contradictory\""},{"location":"changelog/#v172-comparing-self-to-better","text":"Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a binlog event, leading to crashes or in some cases data mismatches.","title":"v1.7.2: \"comparing self to better\""},{"location":"changelog/#v171-blame-it-on-your-seratonin","text":"bootstrapping now can take a --where clause performance improvements in the kafka producer","title":"v1.7.1: \"blame it on your seratonin\""},{"location":"changelog/#v170-lucky-me-lucky-mud","text":"Maxwell 1.7 brings 2 major new, alpha features. The first is Mysql 5.7 support, including JSON column type support and handling of 5.7 SQL, but not including GTID support yet. This is based on porting Maxwell to Stanley Shyko's binlog-connector library. Thanks to Stanley for his amazing support doing this port. The second major new feature is a producer for Amazon's Kinesis streams, This was contributed in full by the dogged and persistent Thomas Dziedzic. Check it out with --producer=kinesis . There's also some bugfixes: - Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson - allow CHECK() statements inside column definitions","title":"v1.7.0: \"lucky me, lucky mud\""},{"location":"changelog/#v160-give-me-a-quest","text":"This is mostly a bugfix release, but it gets a minor version bump due to a single change of behavior: dates and timestamps which mysql may accept, but are considered invalid (0000-00-00 is a notable example) previously had inconsistent behavior. Now we convert these to NULL. Other bugfixes: - heartbeats have moved into their own table - more fixes around alibaba rds - ignore DELETE statements that are output for MEMORY tables upon server restart - allow pointing maxwell to a pre-existing database","title":"v1.6.0: \"give me a quest\""},{"location":"changelog/#v152-french-banana","text":"add support for kafka 0.10.1 @ smferguson master recovery: cleanup positions from previous master; prevent errors on flip-back. fix a bug that would trigger in certain cases when dropping a column that was part of the primary-key","title":"v1.5.2: \"french banana\""},{"location":"changelog/#v151-151-is-just-151","text":"This is a bugfix release. - fixes for bootstrapping with an alternative maxwell-schema name and an include_database filter, thanks Lucian Jones - fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson - ignore the RDS table mysql.ha_health_check table - Get the bootstrapping process to output NULL values. - fix a quoting issue in the bootstrap code, thanks @mylesjao.","title":"v1.5.1: \"1.5.1 is just 1.5.1\""},{"location":"changelog/#v150-someone-somewhere-is-still-smoking-cigarettes-damnit","text":"CHANGE: Kafka producer no longer ships with hard-coded defaults. Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\" configured to your liking. bugfix: fix a regression in handling ALTER TABLE change c int after b statements warn on servers with missing server_id","title":"v1.5.0: \"someone, somewhere, is still smoking cigarettes, damnit\""},{"location":"changelog/#v142-drawer-cat-is-back","text":"kafka 0.10.0 support, as well as a re-working of the --kafka_version command line option.","title":"v1.4.2: \"drawer cat is back\""},{"location":"changelog/#v141-cat-snores","text":"support per-table topics, Thanks @smferguson and @sschatts. fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson","title":"v1.4.1: \"cat snores\""},{"location":"changelog/#v140-deep-insomniac-character-flaws","text":"1.4.0 brings us two nice new features: - partition-by-column: see --kafka_partition_columns. Thanks @smferguson - output schema changes as JSON: see --output_ddl. Thanks @xmlking - As well as a fix around race conditions on shutdown.","title":"v1.4.0: \"deep, insomniac character flaws\""},{"location":"changelog/#v130-yogg-saron","text":"support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan support for outputting server_id & thread_id, thanks @sagiba fix a race condition in bootstrap support","title":"v1.3.0: \"yogg-saron\""},{"location":"changelog/#v122-bats-wearing-frog-pajamas","text":"Maxwell will now include by default fields with NULL values (as null fields). To disable this and restore the old functionality where fields were omitted, pass --output_nulls=false Fix an issue with multi-client support where two replicators would ping-pong heartbeats at each other Fix an issue where a client would attempt to recover a position from a mismatched client_id Fix a bug when using CHANGE COLUMN on a primary key","title":"v1.2.2: \"bats wearing frog pajamas\""},{"location":"changelog/#v121-point-ones-are-a-sad-and-inevitable-fact","text":"This is a bugfix release. - fix a parser bug around ALTER TABLE CHARACTER SET - fix bin/maxwell to pull in the proper version of the kafka-clients library","title":"v1.2.1: \"point-ones are a sad and inevitable fact\""},{"location":"changelog/#v120-just-here-not-to-talk-to-you","text":"1.2.0 is a major release of Maxwell that introduces master recovery features; when a slave is promoted to master, Maxwell is now capable of recovering the position. See the --master_recovery flag for more details. It also upgrades the kafka producer library to 0.9. If you're using maxwell with a kafka 0.8 server, you must now pass the --kafka0.8 flag to maxwell.","title":"v1.2.0: \"just here, not to talk to you\""},{"location":"changelog/#v116-pithy","text":"minor bugfix in which maxwell with --replay mode was trying to write heartbeats","title":"v1.1.6: \"pithy\""},{"location":"changelog/#v115-my-brain-is-a-polluted-mess","text":"@dadah89 adds --output_binlog_position to optionally output the position with the row @dadah89 adds --output_commit_info to turn off xid/commit fields maxwell now supports tables with partitions maxwell now supports N maxwells per-server. see the client_id / replica_server_id options. two parser fixes, for engine= innodb and CHARSET ASCII lay the ground work for doing master recovery; we add a heartbeat into the positions table that we can co-ordinate around.","title":"v1.1.5: \"my brain is a polluted mess\""},{"location":"changelog/#v114-george-flunk","text":"add support for a bunch more charsets (gbk, big5, notably) fix Maxwell's handling of kafka errors - previously we were trying to crash Maxwell by throwing a RuntimeException out of the Kafka Producer, but this was a failure. Now we log and skip all errors.","title":"v1.1.4: \"george flunk\""},{"location":"changelog/#v113-the-button-i-push-to-not-have-to-go-out","text":"This is a bugfix release, which fixes: - https://github.com/zendesk/maxwell/issues/376, a problem parsing RENAME INDEX - https://github.com/zendesk/maxwell/issues/371, a problem with the SERIAL datatype - https://github.com/zendesk/maxwell/issues/362, we now preserve the original casing of columns - https://github.com/zendesk/maxwell/issues/373, we were incorrectly expecting heartbeats to work under 5.1","title":"v1.1.3: \"the button I push to not have to go out\""},{"location":"changelog/#v112-scribbled-notes-on-red-pages","text":"pick up latest mysql-connector-j, fixes #369 fix an issue where maxwell could skip ahead positions if a leader failed. rework buffering code to be much kinder to the GC and JVM heap in case of very large transactions / rows inside transactions kinder, gentler help text when you specify an option incorrectly","title":"v1.1.2: \"scribbled notes on red pages\""},{"location":"changelog/#v111-scribbled-notes-on-blue-pages","text":"fixes a race condition setting the binlog position that would get maxwell stuck","title":"v1.1.1: scribbled notes on blue pages"},{"location":"changelog/#v110-sleep-away-the-afternoon","text":"much more efficient processing of schema updates storage, especially when dealing with large schemas. @lileeyao added --exclude-columns and the --jdbc_options features @lileeyao added --jdbc_options can now blacklist entire databases new kafka key format available, using a JSON array instead of an object bugfix: unsigned integer columns were captured incorrectly. 1.1 will recapture the schema and attempt to correct the error.","title":"v1.1.0: \"sleep away the afternoon\""},{"location":"changelog/#v110-pre4-buck-buck-buck-buck-buck-buck-ah","text":"Eddie McLean gives some helpful patches around bootstrapping Bugfixes for the patch-up-the-schema code around unsigned ints","title":"v1.1.0-pre4: \"buck buck buck buck buck buck-AH!\""},{"location":"changelog/#v110-pre3","text":"forgot to include some updates that back-patch unsigned column problems","title":"v1.1.0-pre3:"},{"location":"changelog/#v110-pre2-yawn-yawn","text":"fix performance issues when capturing schema in AWS Aurora fix a bug in capturing unsigned integer columns","title":"v1.1.0-pre2: \"yawn yawn\""},{"location":"changelog/#v101-bag-of-oversized-daisies","text":"fixes a parsing bug with CURRENT_TIMESTAMP()","title":"v1.0.1: \"bag of oversized daisies\""},{"location":"changelog/#v100-maxwell-learns-to-speak","text":"Since v0.17.0, Maxwell has gotten: - bootstrapping support - blacklisting for tables - flexible kafka partitioning - replication heartbeats - GEOMETRY columns - a whole lotta lotta bugfixes and I, Osheroff, think the damn thing is stable enough for a 1.0. So there.","title":"v1.0.0: \"Maxwell learns to speak\""},{"location":"changelog/#v100-rc3-cmon-and-take-it","text":"pull in support for replication heartbeats. helps in the flakier network environs.","title":"v1.0.0-RC3: \"C'mon and take it\""},{"location":"changelog/#v100-rc2-same-thing-just-without-the-v","text":"fixes the way ALTER DATABASE charset= was handled adds proper handling of ALTER TABLE CONVERT TO CHARSET","title":"v1.0.0-RC2: \"same thing, just without the v\""},{"location":"changelog/#v100-rc1-richard-buckners-release","text":"modifications to the way the bootstrap utility works fix a race condition crash bug in bootstrapping fix a parser bug","title":"v1.0.0-RC1: \"Richard Buckner's release\""},{"location":"changelog/#v100-pre2-an-embarassment-of-riches","text":"1.0.0-PRE2 brings in a lot of changes that got merged while we were testing out PRE1. so, hey. - Configurable names for the maxwell schema database (Kristian Kaufman) - Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman) - Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman) - support GEOMETRY columns, output as well-known-text - add --blacklist_tables option to fully ignore excessive schema changes (Nicolas Maquet) - bootstrap rows now have 'bootstrap-insert' type","title":"v1.0.0-PRE2: \"an embarassment of riches\""},{"location":"changelog/#v100-pre1-drunk-conversations-with-sober-people","text":"Here we have the preview release of @nmaquet's excellent work around bootstrapping initial versions of mysql tables.","title":"v1.0.0-PRE1: \"drunk conversations with sober people\""},{"location":"changelog/#v0170-wrists-of-william","text":"v0.17 is a large bugfix release with one new feature. - FEATURE: allow specifying an alternative mysql schema-storage server and replication server - BUGFIX: properly handle case-sensitivity by aping the behavior of the master server. Fixes #230. - BUGFIX: parse some forms of CHECK( ... ) statements. Fixes #203. - BUGFIX: many more SQL-parser fixes. We are mostly through some thousands of lines of SQL produced by mysql-test.","title":"v0.17.0: \"wrists of William\""},{"location":"changelog/#v0162-the-best-laid-plans","text":"This is a large-ish bugfix release. - Support, with reservations, binlog_row_image=MINIMAL - parser bug: handle table names that look like floating points - parser bug: fix for entity names that have '.', '\\', etc in them - handle UPPERCASE encoding names - support UCS2 (start trying to operate ok on the mysql-test suite) - use ObjectOutputStream.reset to fix memory leaks when buffering to disk","title":"v0.16.2: \"The best laid plans\""},{"location":"changelog/#v0161-me-and-room-service","text":"This is a bug-fix-roundup release: - support ALTER DATABASE - fix a bunch of parse errors: we've started running mysql-test at maxwell and are fixing up failures. - some modifications to the overflow-to-disk logic; we buffer the input and output, and we fix a memory leak","title":"v0.16.1: \"me and room service\""},{"location":"changelog/#v0160-kristian-kaufmanns-version","text":"Version 0.16.0 introduces a feature where UPDATE statements will now show both the new row image and the old values of the fields that changed. Thanks @kristiankaufmann","title":"v0.16.0: \"Kristian Kaufmann's version\""},{"location":"changelog/#v0150-the-littlest-little-city","text":"fix a parse problem with indices ordered by ASC/DESC","title":"v0.15.0: \"the littlest little city\""},{"location":"changelog/#v0150-rc1-its-later-than-you-think","text":"large transactions now buffer to disk instead of crushing maxwell. support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters","title":"v0.15.0-RC1: \"it's later than you think\""},{"location":"changelog/#v0146-its-about-being-american-sort-of","text":"fix TIME column support fix parsing on millisecond precision column defintions fix CREATE SCHEMA parsing","title":"v0.14.6: \"It's about being American.  Sort of.\""},{"location":"changelog/#v0145-false-is-the-new-true","text":"handle BOOLEAN columns with true/false defaults","title":"v0.14.5: \"false is the new true\""},{"location":"changelog/#v0144-youd-think-wed-be-at-10-by-now-wouldnt-you","text":"fixes parsing of \"mysql comments\" ( /*! .. */ ) More performance improvements, another 10% in a tight loop.","title":"v0.14.4: \"You'd think we'd be at 1.0 by now, wouldn't you?\""},{"location":"changelog/#v0143-peanuts-my-girlfriend-thinks-about-peanuts","text":"fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.","title":"v0.14.3: \"Peanuts.  My girlfriend thinks about peanuts.\""},{"location":"changelog/#v0142-maxwell-sandvik-88","text":"capture the mysql database along with the rest of the schema. Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed.","title":"v0.14.2: \"Maxwell Sandvik 88\""},{"location":"changelog/#v0141-be-liberal-in-what-you-accept-even-if-nonsensical","text":"fixes a parser bug around named PRIMARY KEYs.","title":"v0.14.1: \"be liberal in what you accept.  Even if nonsensical.\""},{"location":"changelog/#v0140-the-slow-but-inevitable-slide","text":"This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.","title":"v0.14.0: \"the slow but inevitable slide\""},{"location":"changelog/#v0131-well-that-was-somewhat-expected","text":"v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. v0.13.0 contains: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.","title":"v0.13.1: \"well that was somewhat expected\""},{"location":"changelog/#v0130-malkovich-malkovich-malkovich-sheldon","text":"Lucky release number 13 brings some reasonably big changes: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. This release has a pretty bad bug. do not use.","title":"v0.13.0: \"Malkovich Malkovich Malkovich Sheldon?\""},{"location":"changelog/#v0120-what-do-i-call-them-slippers-why-are-you-jealous","text":"add support for BIT columns.","title":"v0.12.0: \"what do I call them?  Slippers?  Why, are you jealous?\""},{"location":"changelog/#v0114-13-steps","text":"this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event. I really need to fix this at a lower level, ie the open-replicator level.","title":"v0.11.4: \"13 steps\""},{"location":"changelog/#v0113-and-the-other-half-is-to-take-the-bugs-out","text":"this is a bugfix release: - fix problems with table creation options inside alter statements ( ALTER TABLE foo auto_increment=10 ) - fix a host of shutdown-procedure bugs the test suite should also be way more reliable, not like you care.","title":"v0.11.3: \".. and the other half is to take the bugs out\""},{"location":"changelog/#v0112-savage-acts-of-unprovoked-violence-are-bad","text":"This is a bugfix release. It includes: - soft deletions of maxwell.schemas to fix A->B->A master swapping without creating intense replication delay - detect and fail early if we see binlog_row_image=minimal - kill off maxwell if the position thread dies - fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)","title":"v0.11.2: \"savage acts of unprovoked violence are bad\""},{"location":"changelog/#v0111-dog-snoring-loudly","text":"maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid. this should prevent the worst of cases.","title":"v0.11.1: \"dog snoring loudly\""},{"location":"changelog/#v0110-cat-waving-gently","text":"This release of Maxwell preserves transaction information in the kafka stream by adding a xid key in the JSON object, as well as a commit key for the final row inside the transaction. It also contains a bugfix around server_id handling.","title":"v0.11.0: \"cat waving gently\""},{"location":"changelog/#v0101-all-64-of-your-bases-belong-to-shut-up-internet-parrot","text":"proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded) fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash make table option parsing more lenient","title":"v0.10.1: \"all 64 of your bases belong to... shut up, internet parrot.\""},{"location":"changelog/#v0110-rc1-goin-faster-than-a-rollercoaster","text":"merge master fixes","title":"v0.11.0-RC1: \"goin' faster than a rollercoaster\""},{"location":"changelog/#v0100-the-first-word-is-french","text":"Mysql 5.6 checksum support! some more bugfixes with the SQL parser","title":"v0.10.0: \"The first word is French\""},{"location":"changelog/#v0110-pre4-except-for-that-other-thing","text":"bugfix on v0.11.0-PRE3","title":"v0.11.0-PRE4: \"except for that other thing\""},{"location":"changelog/#v0110-pre3-nothing-like-a-good-nights-sleep","text":"handle SAVEPOINT within transactions downgrade unhandled SQL to a warning","title":"v0.11.0-PRE3: \"nothing like a good night's sleep\""},{"location":"changelog/#v0110-pre2-you-really-need-to-name-a-pre-release-something-cutesy","text":"fixes for myISAM \"transactions\"","title":"v0.11.0-PRE2: \"you really need to name a PRE release something cutesy?\""},{"location":"changelog/#v0110-pre1-a-slow-traffic-jam-towards-the-void","text":"fix a server_id bug (was always 1 in maxwell.schemas) JSON output now includes transaction IDs","title":"v0.11.0-PRE1: \"A slow traffic jam towards the void\""},{"location":"changelog/#v0100-rc4-inspiring-confidence","text":"deal with BINARY flag in string column creation.","title":"v0.10.0-RC4: \"Inspiring confidence\""},{"location":"changelog/#v095-long-story-short-thats-why-im-late","text":"handle the BINARY flag in column creation","title":"v0.9.5: \"Long story short, that's why I'm late\""},{"location":"changelog/#v0100-rc3-except-for-that-one-thing","text":"handle \"TRUNCATE [TABLE_NAME]\" statements","title":"v0.10.0-RC3: \"Except for that one thing\""},{"location":"changelog/#v0100-rc2-rc2-is-always-a-good-sign","text":"fixes a bug with checksum processing.","title":"v0.10.0-RC2: \"RC2 is always a good sign.\""},{"location":"changelog/#v0100-rc1-verify-all-the-things","text":"upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.","title":"v0.10.0-RC1: \"verify all the things\""},{"location":"changelog/#v094-weve-been-here-before","text":"allow a configurable number (including unlimited) of schemas to be stored","title":"v0.9.4: \"we've been here before\""},{"location":"changelog/#v093-some-days-its-just-better-to-stay-in-bed","text":"bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes","title":"v0.9.3: \"some days it's just better to stay in bed\""},{"location":"changelog/#v092-cats-tongue","text":"bump open-replicator buffer to 50mb by default log to STDERR, not STDOUT --output_file option for file producer","title":"v0.9.2: \"Cat's tongue\""},{"location":"changelog/#v091-bugs-bugs-bugs-lies-statistics","text":"Maxwell is now aware that column names are case-insenstive fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.","title":"v0.9.1: \"bugs, bugs, bugs, lies, statistics\""},{"location":"changelog/#v090-vanchi-says-eat","text":"Also, vanchi is so paranoid he's worried immediately about this. mysql 5.6 support (without checksum support, yet) fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)","title":"v0.9.0: Vanchi says \"eat\""},{"location":"changelog/#v081-pascal-says-bonjour","text":"minor bugfix release around mysql connections going away.","title":"v0.8.1: \"Pascal says Bonjour\""},{"location":"changelog/#v080-the-cat-never-shuts-up","text":"add \"ts\" field to row output add --config option for passing a different config file support int1, int2, int4, int8 columns","title":"v0.8.0: the cat never shuts up"},{"location":"changelog/#v072-all-the-sql-ladies","text":"handle inline sql comments ignore more user management SQL","title":"v0.7.2: \"all the sql ladies\""},{"location":"changelog/#v071-not-hoarders","text":"only keep 5 most recent schemas","title":"v0.7.1: \"not hoarders\""},{"location":"changelog/#v070-070-alameda","text":"handle CURRENT_TIMESTAMP parsing properly better binlog position sync behavior","title":"v0.7.0: 0.7.0, \"alameda\""},{"location":"changelog/#v063-063","text":"better blacklist for CREATE TRIGGER","title":"v0.6.3: 0.6.3"},{"location":"changelog/#v062-v062","text":"maxwell now ignores SAVEPOINT statements.","title":"v0.6.2: v0.6.2"},{"location":"changelog/#v061-v061","text":"fixes a bug with parsing length-limited indexes.","title":"v0.6.1: v0.6.1"},{"location":"changelog/#v060-kafkakafkakafa","text":"Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy. It also fixes a couple of bugs in the query parsing path.","title":"v0.6.0: kafkakafkakafa"},{"location":"changelog/#v050-050-people-who-put-commas-in-column-names-deserve-undefined-behavior","text":"maxwell now captures primary keys on tables. We'll use this to form kafka key names later. maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.","title":"v0.5.0: 0.5.0 -- \"People who put commas in column names deserve undefined behavior\""},{"location":"changelog/#v040-040-unboxed-cat","text":"v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.","title":"v0.4.0: 0.4.0, \"unboxed cat\""},{"location":"changelog/#v030-030","text":"This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. It also enables kafka gzip compression by default.","title":"v0.3.0: 0.3.0"},{"location":"changelog/#v022-022","text":"Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling.","title":"v0.2.2: 0.2.2"},{"location":"changelog/#v021-v021","text":"version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.","title":"v0.2.1: v0.2.1"},{"location":"changelog/#v020-020","text":"This release gets Maxwell storing the last-written binlog position inside the mysql master itself.","title":"v0.2.0: 0.2.0"},{"location":"changelog/#v014-014","text":"support --position_file param","title":"v0.1.4: 0.1.4"},{"location":"changelog/#v013-013","text":"Adds kafka command line options.","title":"v0.1.3: 0.1.3"},{"location":"changelog/#v011-011","text":"v0.1.1, a small bugfix release.","title":"v0.1.1: 0.1.1"},{"location":"changelog/#v01-01","text":"This is the first possible release of Maxwell that might work. It includes some exceedingly basic kafka support, and JSON output of binlog deltas.","title":"v0.1: 0.1"},{"location":"compat/","text":"Compability Requirements JRE 7 or above mysql 5.1, 5.5, 5.6, 5.7, 8 kafka 0.8.2 or greater Caveats / Notes binlog_row_image=MINIMAL As of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want. It will differ from normal Maxwell operation in that: INSERT statements will no longer output a column's default value UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs, but data will only include what is needed to perform the update (generally, id columns and changed columns). The old section may or may not be included, depending on the nature of the update. DELETE statements will be incomplete; generally they will only include the primary key. Master recovery As of 1.2.0, maxwell includes experimental support for master position recovery. It works like this: maxwell writes heartbeats into the binlogs (via the positions table) maxwell reads its own heartbeats, using them as a secondary position guide if maxwell boots and can't find its position matching the server_id it's connecting to, it will look for a row in maxwell.positions from a different server_id. if it finds that row, it will scan backwards in the binary logs of the new master until it finds that heartbeat. Notes: master recovery is not compatible with separate schema-store hosts and replication-hosts, due to the heartbeat mechanism. this code should be considered alpha-quality. on highly active servers, as much as 1 second of data may be duplicated. master recovery is not available in GTID-mode. MySQL binlog connector As of 1.11.0, maxwell uses shyiko/mysql-binlog-connector-java as its underlying replication library (previously it was opt-in via --binlog_connector ). This is largely compatible with the previous OpenReplicator implementation, but there are some differences: TIMESTAMP columns are always treated as UTC, regardless of your timezone. See issue #681 for more details.","title":"Compatibility"},{"location":"compat/#compability","text":"","title":"Compability"},{"location":"compat/#requirements","text":"JRE 7 or above mysql 5.1, 5.5, 5.6, 5.7, 8 kafka 0.8.2 or greater","title":"Requirements"},{"location":"compat/#caveats-notes","text":"","title":"Caveats / Notes"},{"location":"compat/#binlog_row_imageminimal","text":"As of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want. It will differ from normal Maxwell operation in that: INSERT statements will no longer output a column's default value UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs, but data will only include what is needed to perform the update (generally, id columns and changed columns). The old section may or may not be included, depending on the nature of the update. DELETE statements will be incomplete; generally they will only include the primary key.","title":"binlog_row_image=MINIMAL"},{"location":"compat/#master-recovery","text":"As of 1.2.0, maxwell includes experimental support for master position recovery. It works like this: maxwell writes heartbeats into the binlogs (via the positions table) maxwell reads its own heartbeats, using them as a secondary position guide if maxwell boots and can't find its position matching the server_id it's connecting to, it will look for a row in maxwell.positions from a different server_id. if it finds that row, it will scan backwards in the binary logs of the new master until it finds that heartbeat. Notes: master recovery is not compatible with separate schema-store hosts and replication-hosts, due to the heartbeat mechanism. this code should be considered alpha-quality. on highly active servers, as much as 1 second of data may be duplicated. master recovery is not available in GTID-mode.","title":"Master recovery"},{"location":"compat/#mysql-binlog-connector","text":"As of 1.11.0, maxwell uses shyiko/mysql-binlog-connector-java as its underlying replication library (previously it was opt-in via --binlog_connector ). This is largely compatible with the previous OpenReplicator implementation, but there are some differences: TIMESTAMP columns are always treated as UTC, regardless of your timezone. See issue #681 for more details.","title":"MySQL binlog connector"},{"location":"config/","text":"Reference At the minimum, you will need to specify 'host', 'user', 'password', 'producer'. The kafka producer requires 'kafka.bootstrap.servers', the kinesis producer requires 'kinesis_stream'. general option argument description default config STRING location of config.properties file $PWD/config.properties log_level LOG_LEVEL log level info daemon running maxwell as a daemon env_config_prefix STRING env vars matching prefix are treated as config values mysql option argument description default host STRING mysql host localhost user STRING mysql username password STRING mysql password (no password) port INT mysql port 3306 jdbc_options STRING mysql jdbc connection options DEFAULT_JDBC_OPTS ssl SSL_OPT SSL behavior for mysql cx DISABLED schema_database STRING database to store schema and position in maxwell client_id STRING unique text identifier for maxwell instance maxwell replica_server_id LONG unique numeric identifier for this maxwell instance 6379 (see notes ) master_recovery BOOLEAN enable experimental master recovery code false gtid_mode BOOLEAN enable GTID-based replication false recapture_schema BOOLEAN recapture the latest schema. Not available in config.properties. false max_schemas LONG how many schema deltas to keep before triggering compaction operation unlimited binlog_heartbeat BOOLEAN enable binlog heartbeats to detect stale connections DISABLED replication_host STRING server to replicate from. See split server roles schema-store host replication_password STRING password on replication server (none) replication_port INT port on replication server 3306 replication_user STRING user on replication server replication_ssl SSL_OPT SSL behavior for replication cx cx DISABLED replication_jdbc_options STRING mysql jdbc connection options for replication server DEFAULT_JDBC_OPTS schema_host STRING server to capture schema from. See split server roles schema-store host schema_password STRING password on schema-capture server (none) schema_port INT port on schema-capture server 3306 schema_user STRING user on schema-capture server schema_ssl SSL_OPT SSL behavior for schema-capture server DISABLED schema_jdbc_options STRING mysql jdbc connection options for schema server DEFAULT_JDBC_OPTS producer options option argument description default producer PRODUCER_TYPE type of producer to use stdout custom_producer.factory CLASS_NAME fully qualified custom producer factory class, see example producer_ack_timeout PRODUCER_ACK_TIMEOUT time in milliseconds before async producers consider a message lost producer_partition_by PARTITION_BY input to kafka/kinesis partition function database producer_partition_columns STRING if partitioning by 'column', a comma separated list of columns producer_partition_by_fallback PARTITION_BY_FALLBACK required when producer_partition_by=column. Used when the column is missing ignore_producer_error BOOLEAN When false, Maxwell will terminate on kafka/kinesis/pubsub publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic true file producer option argument description default output_file STRING output file for file producer javascript STRING file containing javascript filters kafka producer option argument description default kafka.bootstrap.servers STRING kafka brokers, given as HOST:PORT[,HOST:PORT] kafka_topic STRING kafka topic to write to. maxwell dead_letter_topic STRING the topic to write a \"skeleton row\" (a row where data includes only primary key columns) when there's an error publishing a row. When ignore_producer_error is false , only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher kafka_version KAFKA_VERSION run maxwell with specified kafka producer version. Not available in config.properties. 0.11.0.1 kafka_partition_hash [ default | murmur3 ] hash function to use when choosing kafka partition default kafka_key_format [ array | hash ] how maxwell outputs kafka keys, either a hash or an array of hashes hash ddl_kafka_topic STRING if output_ddl is true, kafka topic to write DDL changes to kafka_topic kinesis producer option argument description default kinesis_stream STRING kinesis stream name sqs producer option argument description default sqs_queue_uri STRING SQS Queue URI nats producer option argument description default nats_url STRING Comma separated list of nats urls. may include user:password style auth nats://localhost:4222 nats_subject STRING Nats subject hierarchy. Topic substitution available. %{database}.%{table} pubsub producer option argument description default pubsub_topic STRING Google Cloud pub-sub topic pubsub_platform_id STRING Google Cloud platform id associated with topic ddl_pubsub_topic STRING Google Cloud pub-sub topic to send DDL events to pubsub_request_bytes_threshold LONG Set number of bytes until batch is send 1 pubsub_message_count_batch_size LONG Set number of messages until batch is send 1 pubsub_publish_delay_threshold LONG Set time passed in millis until batch is send 1 pubsub_retry_delay LONG Controls the delay in millis before sending the first retry message 100 pubsub_retry_delay_multiplier FLOAT Controls the increase in retry delay per retry 1.3 pubsub_max_retry_delay LONG Puts a limit on the value in seconds of the retry delay 60 pubsub_initial_rpc_timeout LONG Controls the timeout in seconds for the initial RPC 5 pubsub_rpc_timeout_multiplier FLOAT Controls the change in RPC timeout 1.0 pubsub_max_rpc_timeout LONG Puts a limit on the value in seconds of the RPC timeout 600 pubsub_total_timeout LONG Puts a limit on the value in seconds of the retry delay, so that the RetryDelayMultiplier can't increase the retry delay higher than this amount 600 rabbitmq producer option argument description default rabbitmq_user STRING Username of Rabbitmq connection guest rabbitmq_pass STRING Password of Rabbitmq connection guest rabbitmq_host STRING Host of Rabbitmq machine rabbitmq_port INT Port of Rabbitmq machine rabbitmq_virtual_host STRING Virtual Host of Rabbitmq rabbitmq_exchange STRING Name of exchange for rabbitmq publisher rabbitmq_exchange_type STRING Exchange type for rabbitmq rabbitmq_exchange_durable BOOLEAN Exchange durability. false rabbitmq_exchange_autodelete BOOLEAN If set, the exchange is deleted when all queues have finished using it. false rabbitmq_routing_key_template STRING A string template for the routing key, %db% and %table% will be substituted. %db%.%table% . rabbitmq_message_persistent BOOLEAN Eanble message persistence. false rabbitmq_declare_exchange BOOLEAN Should declare the exchange for rabbitmq publisher true redis producer option argument description default redis_host STRING Host of Redis server localhost redis_port INT Port of Redis server 6379 redis_auth STRING Authentication key for a password-protected Redis server redis_database INT Database of Redis server 0 redis_type [ pubsub | xadd | lpush | rpush ] Selects either Redis Pub/Sub, Stream, or List. pubsub redis_key STRING Redis channel/key for Pub/Sub, XADD or LPUSH/RPUSH maxwell redis_stream_json_key STRING Redis XADD Stream Message Field Name message redis_sentinels STRING Redis sentinels list in format host1:port1,host2:port2,host3:port3... Must be only used with redis_sentinel_master_name redis_sentinel_master_name STRING Redis sentinel master name. Must be only used with redis_sentinels formatting option argument description default output_binlog_position BOOLEAN records include binlog position false output_gtid_position BOOLEAN records include gtid position, if available false output_commit_info BOOLEAN records include commit and xid true output_xoffset BOOLEAN records include virtual tx-row offset false output_push_timestamp BOOLEAN records are timestamped with a high-precision value before being sent to the producer false output_nulls BOOLEAN records include fields with NULL values true output_server_id BOOLEAN records include server_id false output_thread_id BOOLEAN records include thread_id false output_schema_id BOOLEAN records include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value false output_row_query BOOLEAN records include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled false output_primary_keys BOOLEAN DML records include list of values that make up a row's primary key false output_primary_key_columns BOOLEAN DML records include list of columns that make up a row's primary key false output_ddl BOOLEAN output DDL (table-alter, table-create, etc) events false output_null_zerodates BOOLEAN should we transform '0000-00-00' to null? false output_naming_strategy STRING naming strategy of field name of JSON. can be underscore_to_camelcase none filtering option argument description default filter STRING filter rules, eg exclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val encryption option argument description default encrypt [ none | data | all ] encrypt mode: none = no encryption. \"data\": encrypt the data field only. all : encrypt entire maxwell message none secret_key string specify the encryption key to be used null high availability option argument description default ha enable maxwell client HA jgroups_config string location of xml configuration file for jGroups $PWD/raft.xml raft_member_id string uniquely identify this node within jgroups-raft cluster monitoring / metrics option argument description default metrics_prefix STRING the prefix maxwell will apply to all metrics MaxwellMetrics metrics_type [slf4j | jmx | http | datadog] how maxwell metrics will be reported metrics_jvm BOOLEAN enable jvm metrics: memory usage, GC stats, etc. false metrics_slf4j_interval SECONDS the frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured 60 http_port INT the port the server will bind to when http reporting is configured 8080 http_path_prefix STRING http path prefix for the server / http_bind_address STRING the address the server will bind to when http reporting is configured all addresses http_diagnostic BOOLEAN enable http diagnostic endpoint false http_diagnostic_timeout MILLISECONDS the http diagnostic response timeout 10000 metrics_datadog_type [udp | http] when metrics_type includes datadog this is the way metrics will be reported, can only be one of [udp | http] udp metrics_datadog_tags STRING datadog tags that should be supplied, e.g. tag1:value1,tag2:value2 metrics_age_slo INT Latency service level objective threshold in seconds (Optional). When set, a message.publish.age.slo_violation metric is emitted to Datadog if the latency exceeds the threshold metrics_datadog_interval INT the frequency metrics are pushed to datadog, in seconds 60 metrics_datadog_apikey STRING the datadog api key to use when metrics_datadog_type = http metrics_datadog_site STRING the site to publish metrics to when metrics_datadog_type = http us metrics_datadog_host STRING the host to publish metrics to when metrics_datadog_type = udp localhost metrics_datadog_port INT the port to publish metrics to when metrics_datadog_type = udp 8125 misc option argument description default bootstrapper [async | sync | none] bootstrapper type. See bootstrapping docs. async init_position FILE:POSITION[:HEARTBEAT] ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties. replay BOOLEAN enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes. Not available in config.properties. buffer_memory_usage FLOAT Determines how much memory the Maxwell event buffer will use from the jvm max memory. Size of the buffer is: buffer_memory_usage * -Xmx\" 0.25 LOG_LEVEL: [ debug | info | warn | error ] SSL_OPTION: [ DISABLED | PREFERRED | REQUIRED | VERIFY_CA | VERIFY_IDENTITY ] PRODUCER_TYPE: [ stdout | file | kafka | kinesis | pubsub | sqs | rabbitmq | redis ] DEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull&connectTimeout=5000 PARTITION_BY: [ database | table | primary_key | transaction_id | column | random ] PARTITION_BY_FALLBACK: [ database | table | primary_key | transaction_id ] KAFKA_VERSION: [ 0.8.2.2 | 0.9.0.1 | 0.10.0.1 | 0.10.2.1 | 0.11.0.1 ] PRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear a message, never notifying maxwell of success or failure. This timeout can be set as a heuristic; after this many milliseconds, maxwell will consider an outstanding message lost and fail it. Configuration methods Maxwell is configurable via the command-line, a properties file, or the environment. The configuration priority is: command line options > scoped env vars > properties file > default values config.properties Maxwell can be configured via a java properties file, specified via --config or named \"config.properties\" in the current working directory. Any command line options (except init_position , replay , kafka_version and daemon ) may be specified as \"key=value\" pairs. via environment If env_config_prefix given via command line or in config.properties , Maxwell will configure itself with all environment variables that match the prefix. The environment variable names are case insensitive. For example, if maxwell is started with --env_config_prefix=FOO_ and the environment contains FOO_USER=auser , this would be equivalent to passing --user=auser . Deployment scenarios At a minimum, Maxwell needs row-level-replication turned on into order to operate: [mysqld] server_id=1 log-bin=master binlog_format=row GTID support As of 1.8.0, Maxwell contains support for GTID-based replication . Enable it with the --gtid_mode configuration param. Here's how you might configure your mysql server for GTID mode: $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row gtid-mode=ON log-slave-updates=ON enforce-gtid-consistency=true When in GTID-mode, Maxwell will transparently pick up a new replication position after a master change. Note that you will still have to re-point maxwell to the new master. GTID support in Maxwell is considered beta-quality at the moment; notably, Maxwell is unable to transparently upgrade from a traditional-replication scenario to a GTID-replication scenario; currently, when you enable gtid mode Maxwell will recapture the schema and GTID-position from \"wherever the master is at\". RDS configuration To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following: set binlog_format to \"ROW\". Do this in the \"parameter groups\" section. For a Mysql-RDS instance this parameter will be in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\". setup RDS binlog retention as described here . The tl;dr is to execute call mysql.rds_set_configuration('binlog retention hours', 24) on the server. Split server roles Maxwell uses MySQL for 3 different functions: A host to store the captured schema in ( --host ). A host to replicate from ( --replication_host ). A host to capture the schema from ( --schema_host ). Often, all three hosts are the same. host and replication_host should be different if maxwell is chained off a replica. schema_host should only be used when using the maxscale replication proxy. Multiple Maxwell Instances Maxwell can operate with multiple instances running against a single master, in different configurations. This can be useful if you wish to have producers running in different configurations, for example producing different groups of tables to different topics. Each instance of Maxwell must be configured with a unique client_id , in order to store unique binlog positions. With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must also be configured with a unique replica_server_id . This is a 32-bit integer that corresponds to mysql's server_id parameter. The value you configure should be unique across all mysql and maxwell instances.","title":"Reference"},{"location":"config/#reference","text":"At the minimum, you will need to specify 'host', 'user', 'password', 'producer'. The kafka producer requires 'kafka.bootstrap.servers', the kinesis producer requires 'kinesis_stream'.","title":"Reference"},{"location":"config/#general","text":"option argument description default config STRING location of config.properties file $PWD/config.properties log_level LOG_LEVEL log level info daemon running maxwell as a daemon env_config_prefix STRING env vars matching prefix are treated as config values","title":"general"},{"location":"config/#mysql","text":"option argument description default host STRING mysql host localhost user STRING mysql username password STRING mysql password (no password) port INT mysql port 3306 jdbc_options STRING mysql jdbc connection options DEFAULT_JDBC_OPTS ssl SSL_OPT SSL behavior for mysql cx DISABLED schema_database STRING database to store schema and position in maxwell client_id STRING unique text identifier for maxwell instance maxwell replica_server_id LONG unique numeric identifier for this maxwell instance 6379 (see notes ) master_recovery BOOLEAN enable experimental master recovery code false gtid_mode BOOLEAN enable GTID-based replication false recapture_schema BOOLEAN recapture the latest schema. Not available in config.properties. false max_schemas LONG how many schema deltas to keep before triggering compaction operation unlimited binlog_heartbeat BOOLEAN enable binlog heartbeats to detect stale connections DISABLED replication_host STRING server to replicate from. See split server roles schema-store host replication_password STRING password on replication server (none) replication_port INT port on replication server 3306 replication_user STRING user on replication server replication_ssl SSL_OPT SSL behavior for replication cx cx DISABLED replication_jdbc_options STRING mysql jdbc connection options for replication server DEFAULT_JDBC_OPTS schema_host STRING server to capture schema from. See split server roles schema-store host schema_password STRING password on schema-capture server (none) schema_port INT port on schema-capture server 3306 schema_user STRING user on schema-capture server schema_ssl SSL_OPT SSL behavior for schema-capture server DISABLED schema_jdbc_options STRING mysql jdbc connection options for schema server DEFAULT_JDBC_OPTS","title":"mysql"},{"location":"config/#producer-options","text":"option argument description default producer PRODUCER_TYPE type of producer to use stdout custom_producer.factory CLASS_NAME fully qualified custom producer factory class, see example producer_ack_timeout PRODUCER_ACK_TIMEOUT time in milliseconds before async producers consider a message lost producer_partition_by PARTITION_BY input to kafka/kinesis partition function database producer_partition_columns STRING if partitioning by 'column', a comma separated list of columns producer_partition_by_fallback PARTITION_BY_FALLBACK required when producer_partition_by=column. Used when the column is missing ignore_producer_error BOOLEAN When false, Maxwell will terminate on kafka/kinesis/pubsub publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic true","title":"producer options"},{"location":"config/#file-producer","text":"option argument description default output_file STRING output file for file producer javascript STRING file containing javascript filters","title":"file producer"},{"location":"config/#kafka-producer","text":"option argument description default kafka.bootstrap.servers STRING kafka brokers, given as HOST:PORT[,HOST:PORT] kafka_topic STRING kafka topic to write to. maxwell dead_letter_topic STRING the topic to write a \"skeleton row\" (a row where data includes only primary key columns) when there's an error publishing a row. When ignore_producer_error is false , only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher kafka_version KAFKA_VERSION run maxwell with specified kafka producer version. Not available in config.properties. 0.11.0.1 kafka_partition_hash [ default | murmur3 ] hash function to use when choosing kafka partition default kafka_key_format [ array | hash ] how maxwell outputs kafka keys, either a hash or an array of hashes hash ddl_kafka_topic STRING if output_ddl is true, kafka topic to write DDL changes to kafka_topic","title":"kafka producer"},{"location":"config/#kinesis-producer","text":"option argument description default kinesis_stream STRING kinesis stream name","title":"kinesis producer"},{"location":"config/#sqs-producer","text":"option argument description default sqs_queue_uri STRING SQS Queue URI","title":"sqs producer"},{"location":"config/#nats-producer","text":"option argument description default nats_url STRING Comma separated list of nats urls. may include user:password style auth nats://localhost:4222 nats_subject STRING Nats subject hierarchy. Topic substitution available. %{database}.%{table}","title":"nats producer"},{"location":"config/#pubsub-producer","text":"option argument description default pubsub_topic STRING Google Cloud pub-sub topic pubsub_platform_id STRING Google Cloud platform id associated with topic ddl_pubsub_topic STRING Google Cloud pub-sub topic to send DDL events to pubsub_request_bytes_threshold LONG Set number of bytes until batch is send 1 pubsub_message_count_batch_size LONG Set number of messages until batch is send 1 pubsub_publish_delay_threshold LONG Set time passed in millis until batch is send 1 pubsub_retry_delay LONG Controls the delay in millis before sending the first retry message 100 pubsub_retry_delay_multiplier FLOAT Controls the increase in retry delay per retry 1.3 pubsub_max_retry_delay LONG Puts a limit on the value in seconds of the retry delay 60 pubsub_initial_rpc_timeout LONG Controls the timeout in seconds for the initial RPC 5 pubsub_rpc_timeout_multiplier FLOAT Controls the change in RPC timeout 1.0 pubsub_max_rpc_timeout LONG Puts a limit on the value in seconds of the RPC timeout 600 pubsub_total_timeout LONG Puts a limit on the value in seconds of the retry delay, so that the RetryDelayMultiplier can't increase the retry delay higher than this amount 600","title":"pubsub producer"},{"location":"config/#rabbitmq-producer","text":"option argument description default rabbitmq_user STRING Username of Rabbitmq connection guest rabbitmq_pass STRING Password of Rabbitmq connection guest rabbitmq_host STRING Host of Rabbitmq machine rabbitmq_port INT Port of Rabbitmq machine rabbitmq_virtual_host STRING Virtual Host of Rabbitmq rabbitmq_exchange STRING Name of exchange for rabbitmq publisher rabbitmq_exchange_type STRING Exchange type for rabbitmq rabbitmq_exchange_durable BOOLEAN Exchange durability. false rabbitmq_exchange_autodelete BOOLEAN If set, the exchange is deleted when all queues have finished using it. false rabbitmq_routing_key_template STRING A string template for the routing key, %db% and %table% will be substituted. %db%.%table% . rabbitmq_message_persistent BOOLEAN Eanble message persistence. false rabbitmq_declare_exchange BOOLEAN Should declare the exchange for rabbitmq publisher true","title":"rabbitmq producer"},{"location":"config/#redis-producer","text":"option argument description default redis_host STRING Host of Redis server localhost redis_port INT Port of Redis server 6379 redis_auth STRING Authentication key for a password-protected Redis server redis_database INT Database of Redis server 0 redis_type [ pubsub | xadd | lpush | rpush ] Selects either Redis Pub/Sub, Stream, or List. pubsub redis_key STRING Redis channel/key for Pub/Sub, XADD or LPUSH/RPUSH maxwell redis_stream_json_key STRING Redis XADD Stream Message Field Name message redis_sentinels STRING Redis sentinels list in format host1:port1,host2:port2,host3:port3... Must be only used with redis_sentinel_master_name redis_sentinel_master_name STRING Redis sentinel master name. Must be only used with redis_sentinels","title":"redis producer"},{"location":"config/#formatting","text":"option argument description default output_binlog_position BOOLEAN records include binlog position false output_gtid_position BOOLEAN records include gtid position, if available false output_commit_info BOOLEAN records include commit and xid true output_xoffset BOOLEAN records include virtual tx-row offset false output_push_timestamp BOOLEAN records are timestamped with a high-precision value before being sent to the producer false output_nulls BOOLEAN records include fields with NULL values true output_server_id BOOLEAN records include server_id false output_thread_id BOOLEAN records include thread_id false output_schema_id BOOLEAN records include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value false output_row_query BOOLEAN records include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled false output_primary_keys BOOLEAN DML records include list of values that make up a row's primary key false output_primary_key_columns BOOLEAN DML records include list of columns that make up a row's primary key false output_ddl BOOLEAN output DDL (table-alter, table-create, etc) events false output_null_zerodates BOOLEAN should we transform '0000-00-00' to null? false output_naming_strategy STRING naming strategy of field name of JSON. can be underscore_to_camelcase none","title":"formatting"},{"location":"config/#filtering","text":"option argument description default filter STRING filter rules, eg exclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val","title":"filtering"},{"location":"config/#encryption","text":"option argument description default encrypt [ none | data | all ] encrypt mode: none = no encryption. \"data\": encrypt the data field only. all : encrypt entire maxwell message none secret_key string specify the encryption key to be used null","title":"encryption"},{"location":"config/#high-availability","text":"option argument description default ha enable maxwell client HA jgroups_config string location of xml configuration file for jGroups $PWD/raft.xml raft_member_id string uniquely identify this node within jgroups-raft cluster","title":"high availability"},{"location":"config/#monitoring-metrics","text":"option argument description default metrics_prefix STRING the prefix maxwell will apply to all metrics MaxwellMetrics metrics_type [slf4j | jmx | http | datadog] how maxwell metrics will be reported metrics_jvm BOOLEAN enable jvm metrics: memory usage, GC stats, etc. false metrics_slf4j_interval SECONDS the frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured 60 http_port INT the port the server will bind to when http reporting is configured 8080 http_path_prefix STRING http path prefix for the server / http_bind_address STRING the address the server will bind to when http reporting is configured all addresses http_diagnostic BOOLEAN enable http diagnostic endpoint false http_diagnostic_timeout MILLISECONDS the http diagnostic response timeout 10000 metrics_datadog_type [udp | http] when metrics_type includes datadog this is the way metrics will be reported, can only be one of [udp | http] udp metrics_datadog_tags STRING datadog tags that should be supplied, e.g. tag1:value1,tag2:value2 metrics_age_slo INT Latency service level objective threshold in seconds (Optional). When set, a message.publish.age.slo_violation metric is emitted to Datadog if the latency exceeds the threshold metrics_datadog_interval INT the frequency metrics are pushed to datadog, in seconds 60 metrics_datadog_apikey STRING the datadog api key to use when metrics_datadog_type = http metrics_datadog_site STRING the site to publish metrics to when metrics_datadog_type = http us metrics_datadog_host STRING the host to publish metrics to when metrics_datadog_type = udp localhost metrics_datadog_port INT the port to publish metrics to when metrics_datadog_type = udp 8125","title":"monitoring / metrics"},{"location":"config/#misc","text":"option argument description default bootstrapper [async | sync | none] bootstrapper type. See bootstrapping docs. async init_position FILE:POSITION[:HEARTBEAT] ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties. replay BOOLEAN enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes. Not available in config.properties. buffer_memory_usage FLOAT Determines how much memory the Maxwell event buffer will use from the jvm max memory. Size of the buffer is: buffer_memory_usage * -Xmx\" 0.25 LOG_LEVEL: [ debug | info | warn | error ] SSL_OPTION: [ DISABLED | PREFERRED | REQUIRED | VERIFY_CA | VERIFY_IDENTITY ] PRODUCER_TYPE: [ stdout | file | kafka | kinesis | pubsub | sqs | rabbitmq | redis ] DEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull&connectTimeout=5000 PARTITION_BY: [ database | table | primary_key | transaction_id | column | random ] PARTITION_BY_FALLBACK: [ database | table | primary_key | transaction_id ] KAFKA_VERSION: [ 0.8.2.2 | 0.9.0.1 | 0.10.0.1 | 0.10.2.1 | 0.11.0.1 ] PRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear a message, never notifying maxwell of success or failure. This timeout can be set as a heuristic; after this many milliseconds, maxwell will consider an outstanding message lost and fail it.","title":"misc"},{"location":"config/#configuration-methods","text":"Maxwell is configurable via the command-line, a properties file, or the environment. The configuration priority is: command line options > scoped env vars > properties file > default values","title":"Configuration methods"},{"location":"config/#configproperties","text":"Maxwell can be configured via a java properties file, specified via --config or named \"config.properties\" in the current working directory. Any command line options (except init_position , replay , kafka_version and daemon ) may be specified as \"key=value\" pairs.","title":"config.properties"},{"location":"config/#via-environment","text":"If env_config_prefix given via command line or in config.properties , Maxwell will configure itself with all environment variables that match the prefix. The environment variable names are case insensitive. For example, if maxwell is started with --env_config_prefix=FOO_ and the environment contains FOO_USER=auser , this would be equivalent to passing --user=auser .","title":"via environment"},{"location":"config/#deployment-scenarios","text":"At a minimum, Maxwell needs row-level-replication turned on into order to operate: [mysqld] server_id=1 log-bin=master binlog_format=row","title":"Deployment scenarios"},{"location":"config/#gtid-support","text":"As of 1.8.0, Maxwell contains support for GTID-based replication . Enable it with the --gtid_mode configuration param. Here's how you might configure your mysql server for GTID mode: $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row gtid-mode=ON log-slave-updates=ON enforce-gtid-consistency=true When in GTID-mode, Maxwell will transparently pick up a new replication position after a master change. Note that you will still have to re-point maxwell to the new master. GTID support in Maxwell is considered beta-quality at the moment; notably, Maxwell is unable to transparently upgrade from a traditional-replication scenario to a GTID-replication scenario; currently, when you enable gtid mode Maxwell will recapture the schema and GTID-position from \"wherever the master is at\".","title":"GTID support"},{"location":"config/#rds-configuration","text":"To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following: set binlog_format to \"ROW\". Do this in the \"parameter groups\" section. For a Mysql-RDS instance this parameter will be in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\". setup RDS binlog retention as described here . The tl;dr is to execute call mysql.rds_set_configuration('binlog retention hours', 24) on the server.","title":"RDS configuration"},{"location":"config/#split-server-roles","text":"Maxwell uses MySQL for 3 different functions: A host to store the captured schema in ( --host ). A host to replicate from ( --replication_host ). A host to capture the schema from ( --schema_host ). Often, all three hosts are the same. host and replication_host should be different if maxwell is chained off a replica. schema_host should only be used when using the maxscale replication proxy.","title":"Split server roles"},{"location":"config/#multiple-maxwell-instances","text":"Maxwell can operate with multiple instances running against a single master, in different configurations. This can be useful if you wish to have producers running in different configurations, for example producing different groups of tables to different topics. Each instance of Maxwell must be configured with a unique client_id , in order to store unique binlog positions. With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must also be configured with a unique replica_server_id . This is a 32-bit integer that corresponds to mysql's server_id parameter. The value you configure should be unique across all mysql and maxwell instances.","title":"Multiple Maxwell Instances"},{"location":"dataformat/","text":"So you ran some sql? create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ); insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; update test.e set m = 5.444, c = now(3) where id = 1; delete from test.e where id = 1; alter table test.e add column torvalds bigint unsigned after m; drop table test.e; Maxwell will produce some output for that. Let's look at it. INSERT mysql> insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; { \"database\":\"test\", \"table\":\"e\", \"type\":\"insert\", \"ts\":1477053217, \"xid\":23396, \"commit\":true, \"position\":\"master.000006:800911\", \"server_id\":23042, \"thread_id\":108, \"primary_key\": [1, \"2016-10-21 05:33:37.523000\"], \"primary_key_columns\": [\"id\", \"c\"], \"data\":{ \"id\":1, \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\", \"comment\":\"I am a creature of light.\" } } Most of the fields are self-explanatory, but a couple of them deserve mention: \u21b3 \"type\":\"insert\", Most commonly you will see insert/update/delete here. If you're bootstrapping a table, you will see \"bootstrap-insert\", and DDL statements (explained later) have their own types. \u21b3 \"xid\":23396, This is InnoDB's \"transaction ID\" for the transaction this row is associated with. It's unique within the lifetime of a server as near as I can tell. \u21b3 \"server_id\":23042, The mysql server_id of the server that accepted this transaction. \u21b3 \"thread_id\":108, A thread_id is more or less a unique identifier of the client connection that generated the data. \u21b3 \"commit\":true, If you need to re-assemble transactions in your stream processors, you can use this field and xid to do so. The data will look like: row with no commit , xid=142 row with no commit , xid=142 row with commit=true , xid=142 row with no commit , xid=155 ... \u21b3 \"primary_key\": [1,\"2016-10-21 05:33:37.523000\"], You only get this with --output_primary_key. List of values that make up the primary key for this row. \u21b3 \"primary_key_columns\": [\"id\",\"c\"], You only get this with --output_primary_key_columns. List of columns that make make up the primary key for this row. UPDATE mysql> update test.e set m = 5.444, c = now(3) where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"update\", \"ts\":1477053234, ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" }, \"old\":{ \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\" } } What's important to note here is the old field, which stores old values for rows that changed. So data still has a complete copy of the row (just as with the insert), but now you can reconstruct what the row was by doing data.merge(old) . DELETE mysql> delete from test.e where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"delete\", ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" } } after a DELETE, data contains a copy of the row, just before it shuffled off this mortal coil. CREATE TABLE create table test.e ( ... ) { \"type\":\"table-create\", \"database\":\"test\", \"table\":\"e\", \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053126000, \"sql\":\"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\", \"position\":\"master.000006:800050\" } You only get this with --output_ddl . \u21b3 \"type\": \"table-create\" here you have database-create , database-alter , database-drop , table-create , table-alter , table-drop . \u21b3 \"type\":\"int\", Mostly here we preserve the inbound type of the column. There's a couple of exceptions where we will change the column type, you could read about them in the unalias_type function if you so desired. ALTER TABLE mysql> alter table test.e add column torvalds bigint unsigned after m; { \"type\":\"table-alter\", \"database\":\"test\", \"table\":\"e\", \"old\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"bigint\", \"name\":\"torvalds\", \"signed\":false }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053308000, \"sql\":\"alter table test.e add column torvalds bigint unsigned after m\", \"position\":\"master.000006:804398\" } As with the CREATE TABLE, we have a complete image of the table before-and-after the alter blob (+ binary encoded strings) Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding). datetime Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings. Note that mysql has no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and Maxwell chooses to reproduce these invalid datetimes faithfully, for lack of something better to do. mysql> create table test_datetime ( id int(11), dtcol datetime ); mysql> insert into test_datetime set dtcol='0000-00-00 00:00:00'; <maxwell { \"table\" : \"test_datetime\", \"type\": \"insert\", \"data\": { \"dtcol\": \"0000-00-00 00:00:00\" } } As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns. sets output as JSON arrays. mysql> create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') ); mysql> insert into test_sets set setcol = 'b_val,c_val'; <maxwell { \"table\":\"test_sets\", \"type\":\"insert\", \"data\": { \"setcol\": [\"b_val\", \"c_val\"] } } strings (varchar, text) Maxwell will accept a variety of character encodings, but will always output UTF-8 strings. The following table describes support for mysql's character sets: charset status utf8 supported utf8mb4 supported latin1 supported latin2 supported ascii supported ucs2 supported binary supported (as base64) utf16 supported, not tested in production utf32 supported, not tested in production big5 supported, not tested in production cp850 supported, not tested in production sjis supported, not tested in production hebrew supported, not tested in production tis620 supported, not tested in production euckr supported, not tested in production gb2312 supported, not tested in production greek supported, not tested in production cp1250 supported, not tested in production gbk supported, not tested in production latin5 supported, not tested in production macroman supported, not tested in production cp852 supported, not tested in production cp1251 supported, not tested in production cp866 supported, not tested in production cp1256 supported, not tested in production cp1257 supported, not tested in production dec8 unsupported hp8 unsupported koi8r unsupported swe7 unsupported ujis unsupported koi8u unsupported armscii8 unsupported keybcs2 unsupported macce unsupported latin7 unsupported geostd8 unsupported cp932 unsupported eucjpms unsupported","title":"Data Format"},{"location":"dataformat/#so-you-ran-some-sql","text":"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ); insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; update test.e set m = 5.444, c = now(3) where id = 1; delete from test.e where id = 1; alter table test.e add column torvalds bigint unsigned after m; drop table test.e; Maxwell will produce some output for that. Let's look at it.","title":"So you ran some sql?"},{"location":"dataformat/#insert","text":"mysql> insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; { \"database\":\"test\", \"table\":\"e\", \"type\":\"insert\", \"ts\":1477053217, \"xid\":23396, \"commit\":true, \"position\":\"master.000006:800911\", \"server_id\":23042, \"thread_id\":108, \"primary_key\": [1, \"2016-10-21 05:33:37.523000\"], \"primary_key_columns\": [\"id\", \"c\"], \"data\":{ \"id\":1, \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\", \"comment\":\"I am a creature of light.\" } } Most of the fields are self-explanatory, but a couple of them deserve mention: \u21b3 \"type\":\"insert\", Most commonly you will see insert/update/delete here. If you're bootstrapping a table, you will see \"bootstrap-insert\", and DDL statements (explained later) have their own types. \u21b3 \"xid\":23396, This is InnoDB's \"transaction ID\" for the transaction this row is associated with. It's unique within the lifetime of a server as near as I can tell. \u21b3 \"server_id\":23042, The mysql server_id of the server that accepted this transaction. \u21b3 \"thread_id\":108, A thread_id is more or less a unique identifier of the client connection that generated the data. \u21b3 \"commit\":true, If you need to re-assemble transactions in your stream processors, you can use this field and xid to do so. The data will look like: row with no commit , xid=142 row with no commit , xid=142 row with commit=true , xid=142 row with no commit , xid=155 ... \u21b3 \"primary_key\": [1,\"2016-10-21 05:33:37.523000\"], You only get this with --output_primary_key. List of values that make up the primary key for this row. \u21b3 \"primary_key_columns\": [\"id\",\"c\"], You only get this with --output_primary_key_columns. List of columns that make make up the primary key for this row.","title":"INSERT"},{"location":"dataformat/#update","text":"mysql> update test.e set m = 5.444, c = now(3) where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"update\", \"ts\":1477053234, ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" }, \"old\":{ \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\" } } What's important to note here is the old field, which stores old values for rows that changed. So data still has a complete copy of the row (just as with the insert), but now you can reconstruct what the row was by doing data.merge(old) .","title":"UPDATE"},{"location":"dataformat/#delete","text":"mysql> delete from test.e where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"delete\", ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" } } after a DELETE, data contains a copy of the row, just before it shuffled off this mortal coil.","title":"DELETE"},{"location":"dataformat/#create-table","text":"create table test.e ( ... ) { \"type\":\"table-create\", \"database\":\"test\", \"table\":\"e\", \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053126000, \"sql\":\"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\", \"position\":\"master.000006:800050\" } You only get this with --output_ddl . \u21b3 \"type\": \"table-create\" here you have database-create , database-alter , database-drop , table-create , table-alter , table-drop . \u21b3 \"type\":\"int\", Mostly here we preserve the inbound type of the column. There's a couple of exceptions where we will change the column type, you could read about them in the unalias_type function if you so desired.","title":"CREATE TABLE"},{"location":"dataformat/#alter-table","text":"mysql> alter table test.e add column torvalds bigint unsigned after m; { \"type\":\"table-alter\", \"database\":\"test\", \"table\":\"e\", \"old\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"bigint\", \"name\":\"torvalds\", \"signed\":false }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053308000, \"sql\":\"alter table test.e add column torvalds bigint unsigned after m\", \"position\":\"master.000006:804398\" } As with the CREATE TABLE, we have a complete image of the table before-and-after the alter","title":"ALTER TABLE"},{"location":"dataformat/#blob-binary-encoded-strings","text":"Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).","title":"blob (+ binary encoded strings)"},{"location":"dataformat/#datetime","text":"Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings. Note that mysql has no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and Maxwell chooses to reproduce these invalid datetimes faithfully, for lack of something better to do. mysql> create table test_datetime ( id int(11), dtcol datetime ); mysql> insert into test_datetime set dtcol='0000-00-00 00:00:00'; <maxwell { \"table\" : \"test_datetime\", \"type\": \"insert\", \"data\": { \"dtcol\": \"0000-00-00 00:00:00\" } } As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.","title":"datetime"},{"location":"dataformat/#sets","text":"output as JSON arrays. mysql> create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') ); mysql> insert into test_sets set setcol = 'b_val,c_val'; <maxwell { \"table\":\"test_sets\", \"type\":\"insert\", \"data\": { \"setcol\": [\"b_val\", \"c_val\"] } }","title":"sets"},{"location":"dataformat/#strings-varchar-text","text":"Maxwell will accept a variety of character encodings, but will always output UTF-8 strings. The following table describes support for mysql's character sets: charset status utf8 supported utf8mb4 supported latin1 supported latin2 supported ascii supported ucs2 supported binary supported (as base64) utf16 supported, not tested in production utf32 supported, not tested in production big5 supported, not tested in production cp850 supported, not tested in production sjis supported, not tested in production hebrew supported, not tested in production tis620 supported, not tested in production euckr supported, not tested in production gb2312 supported, not tested in production greek supported, not tested in production cp1250 supported, not tested in production gbk supported, not tested in production latin5 supported, not tested in production macroman supported, not tested in production cp852 supported, not tested in production cp1251 supported, not tested in production cp866 supported, not tested in production cp1256 supported, not tested in production cp1257 supported, not tested in production dec8 unsupported hp8 unsupported koi8r unsupported swe7 unsupported ujis unsupported koi8u unsupported armscii8 unsupported keybcs2 unsupported macce unsupported latin7 unsupported geostd8 unsupported cp932 unsupported eucjpms unsupported","title":"strings (varchar, text)"},{"location":"embedding/","text":"Embedding Maxwell Maxwell typically runs as a command-line program. However, for advanced use it is possible to run maxwell from any JVM-based language. Currently the source of truth is the source code (there is no published API documentation). Pull requests to better document embedded Maxwell uses are welcome. Compatibility caveat Maxwell makes every attempt to remain backwards compatible. However this only applies to the command-line usage - Maxwell's Java API may change without notice. However (and unless otherwise indicated) breaking API changes will result in a type error - i.e. if your code still compiles, then the API has not changed.","title":"Embedding"},{"location":"embedding/#embedding-maxwell","text":"Maxwell typically runs as a command-line program. However, for advanced use it is possible to run maxwell from any JVM-based language. Currently the source of truth is the source code (there is no published API documentation). Pull requests to better document embedded Maxwell uses are welcome.","title":"Embedding Maxwell"},{"location":"embedding/#compatibility-caveat","text":"Maxwell makes every attempt to remain backwards compatible. However this only applies to the command-line usage - Maxwell's Java API may change without notice. However (and unless otherwise indicated) breaking API changes will result in a type error - i.e. if your code still compiles, then the API has not changed.","title":"Compatibility caveat"},{"location":"encryption/","text":"Using encryption When encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key. Values are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message Decryption To decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt(). Examples insert into minimal set account_id =1, text_field='hello' encrypt=none (unencrypted): {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1490115785,\"xid\":153,\"commit\":true,\"data\":{\"id\":1,\"account_id\":1,\"text_field\":\"hello\"}} encrypt=data : {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1504585129,\"xid\":161,\"commit\":true,\"encrypted\":{\"iv\":\"lqiXoTdz6jed3XgJPpa7EQ==\",\"bytes\":\"1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA==\"}} encrypt=all : {\"encrypted\":{\"bytes\":\"iZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA==\",\"iv\":\"XXs6AePsXJWAAIrKyLlR0g==\"}}","title":"Encryption"},{"location":"encryption/#using-encryption","text":"When encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key. Values are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message","title":"Using encryption"},{"location":"encryption/#decryption","text":"To decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt().","title":"Decryption"},{"location":"encryption/#examples","text":"insert into minimal set account_id =1, text_field='hello' encrypt=none (unencrypted): {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1490115785,\"xid\":153,\"commit\":true,\"data\":{\"id\":1,\"account_id\":1,\"text_field\":\"hello\"}} encrypt=data : {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1504585129,\"xid\":161,\"commit\":true,\"encrypted\":{\"iv\":\"lqiXoTdz6jed3XgJPpa7EQ==\",\"bytes\":\"1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA==\"}} encrypt=all : {\"encrypted\":{\"bytes\":\"iZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA==\",\"iv\":\"XXs6AePsXJWAAIrKyLlR0g==\"}}","title":"Examples"},{"location":"filtering/","text":"Basic Filters Maxwell can be configured to filter out updates from specific tables. This is controlled by the --filter command line flag. Example 1 --filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/' This example tells Maxwell to suppress all updates that happen on foodb , except for updates to tbl and any table in foodb matching the regexp /table_\\d+/ . Example 2 Filter options are evaluated in the order specified, so in this example we suppress everything except updates in the db1 database. --filter = 'exclude: *.*, include: db1.*' Column Filters Maxwell can also include/exclude based on column values: --filter = 'exclude: db.tbl.col = reject' will reject any row in db.tbl that contains col and where the stringified value of \"col\" is \"reject\". Column filters are ignored if the specified column is not present, so --filter = 'exclude: *.*.col_a = *' will exclude updates to any table that contains col_a , but include every other table. Blacklisting In rare cases, you may wish to tell Maxwell to completely ignore a database or table, including schema changes. In general, don't use this. If you must use this: --filter = 'blacklist: bad_db.*' Note that once Maxwell has been running with a table or database marked as blacklisted, you must continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop blacklisting a table or database, you will have to drop the maxwell schema first. Also note that this is the feature I most regret writing. Javascript Filters If you need more flexibility than the native filters provide, you can write a small chunk of javascript for Maxwell to pass each row through with --javascript FILE . This file should contain at least a javascript function named process_row . This function will be passed a WrappedRowMap object and is free to make filtering and data munging decisions: function process_row(row) { if ( row.database == \"test\" && row.table == \"bar\" ) { var username = row.data.get(\"username\"); if ( username == \"osheroff\" ) row.suppress(); row.data.put(\"username\", username.toUpperCase()); } } There's a longer example here: https://github.com/zendesk/maxwell/blob/master/src/example/filter.js .","title":"Filtering"},{"location":"filtering/#basic-filters","text":"Maxwell can be configured to filter out updates from specific tables. This is controlled by the --filter command line flag.","title":"Basic Filters"},{"location":"filtering/#example-1","text":"--filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/' This example tells Maxwell to suppress all updates that happen on foodb , except for updates to tbl and any table in foodb matching the regexp /table_\\d+/ .","title":"Example 1"},{"location":"filtering/#example-2","text":"Filter options are evaluated in the order specified, so in this example we suppress everything except updates in the db1 database. --filter = 'exclude: *.*, include: db1.*'","title":"Example 2"},{"location":"filtering/#column-filters","text":"Maxwell can also include/exclude based on column values: --filter = 'exclude: db.tbl.col = reject' will reject any row in db.tbl that contains col and where the stringified value of \"col\" is \"reject\". Column filters are ignored if the specified column is not present, so --filter = 'exclude: *.*.col_a = *' will exclude updates to any table that contains col_a , but include every other table.","title":"Column Filters"},{"location":"filtering/#blacklisting","text":"In rare cases, you may wish to tell Maxwell to completely ignore a database or table, including schema changes. In general, don't use this. If you must use this: --filter = 'blacklist: bad_db.*' Note that once Maxwell has been running with a table or database marked as blacklisted, you must continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop blacklisting a table or database, you will have to drop the maxwell schema first. Also note that this is the feature I most regret writing.","title":"Blacklisting"},{"location":"filtering/#javascript-filters","text":"If you need more flexibility than the native filters provide, you can write a small chunk of javascript for Maxwell to pass each row through with --javascript FILE . This file should contain at least a javascript function named process_row . This function will be passed a WrappedRowMap object and is free to make filtering and data munging decisions: function process_row(row) { if ( row.database == \"test\" && row.table == \"bar\" ) { var username = row.data.get(\"username\"); if ( username == \"osheroff\" ) row.suppress(); row.data.put(\"username\", username.toUpperCase()); } } There's a longer example here: https://github.com/zendesk/maxwell/blob/master/src/example/filter.js .","title":"Javascript Filters"},{"location":"high_availability/","text":"High Availabilty As v1.29.1, Maxwell contains experiemental (alpha quality) client-side HA. Support for performing leader elections is done via jgroups-raft . Getting started First, copy raft.xml.example to raft.xml . Edit to your liking, paying attention to: <raft.RAFT members=\"A,B,C\" raft_id=\"${raft_id:undefined}\"/> Note that because we are using a RAFT-based leader election, we will have to spin up at least 3 maxwell client nodes. Now start each of your HA maxwell nodes like this: host1: $ bin/maxwell --ha --raft_member_id=A host2: $ bin/maxwell --ha --raft_member_id=B host3: $ bin/maxwell --ha --raft_member_id=C if all goes well, the 3 nodes will communicate via multicast/UDP, elect one to be the cluster leader, and away you will go. If one node is terminated or partitioned, a new election will be held to replace it. Getting deeper More advanced (especially inter-DC) configurations may be implemented by editing raft.xml ; you'll probably need to get the nodes to communicate with each other via TCP instead of UDP, and maybe tunnel through a firewall or two, good stuff like that. It's of course out of scope for this document, so check out the jgroups documentation , but if you come up with something good drop me a line. Common problems Something I encountered right out of the gate was this: 12:37:53,135 WARN UDP - failed to join /224.0.75.75:7500 on utun0: java.net.SocketException: Can't assign requested address which can be worked around by forcing the JVM onto an ipv4 stack: JAVA_OPTS=\"-Djava.net.preferIPv4Stack=true\" bin/maxwell --ha --raft_member_id=B","title":"High Availability"},{"location":"high_availability/#high-availabilty","text":"As v1.29.1, Maxwell contains experiemental (alpha quality) client-side HA. Support for performing leader elections is done via jgroups-raft .","title":"High Availabilty"},{"location":"high_availability/#getting-started","text":"First, copy raft.xml.example to raft.xml . Edit to your liking, paying attention to: <raft.RAFT members=\"A,B,C\" raft_id=\"${raft_id:undefined}\"/> Note that because we are using a RAFT-based leader election, we will have to spin up at least 3 maxwell client nodes. Now start each of your HA maxwell nodes like this: host1: $ bin/maxwell --ha --raft_member_id=A host2: $ bin/maxwell --ha --raft_member_id=B host3: $ bin/maxwell --ha --raft_member_id=C if all goes well, the 3 nodes will communicate via multicast/UDP, elect one to be the cluster leader, and away you will go. If one node is terminated or partitioned, a new election will be held to replace it.","title":"Getting started"},{"location":"high_availability/#getting-deeper","text":"More advanced (especially inter-DC) configurations may be implemented by editing raft.xml ; you'll probably need to get the nodes to communicate with each other via TCP instead of UDP, and maybe tunnel through a firewall or two, good stuff like that. It's of course out of scope for this document, so check out the jgroups documentation , but if you come up with something good drop me a line.","title":"Getting deeper"},{"location":"high_availability/#common-problems","text":"Something I encountered right out of the gate was this: 12:37:53,135 WARN UDP - failed to join /224.0.75.75:7500 on utun0: java.net.SocketException: Can't assign requested address which can be worked around by forcing the JVM onto an ipv4 stack: JAVA_OPTS=\"-Djava.net.preferIPv4Stack=true\" bin/maxwell --ha --raft_member_id=B","title":"Common problems"},{"location":"monitoring/","text":"Monitoring Maxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options or the config.properties file. These can provide insight into system health. Metrics All metrics are prepended with the configured metrics_prefix. metric description Counters messages.succeeded count of messages that were successfully sent to Kafka messages.failed count of messages that failed to send to Kafka row.count a count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka. Meters messages.succeeded.meter a measure of the rate at which messages were successfully sent to Kafka messages.failed.meter a measure of the rate at which messages failed to send Kafka row.meter a measure of the rate at which rows arrive to Maxwell from the binlog connector Gauges replication.lag the time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds inflightmessages.count the number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are) Timers message.publish.time the time it took to send a given record to Kafka, in milliseconds message.publish.age the time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms. replication.queue.time the time it took to enqueue a given binlog event for processing, in milliseconds HTTP Endpoints When the HTTP server is enabled the following endpoints are exposed: endpoint description /metrics return all metrics as JSON /prometheus return all metrics as Prometheus format /healthcheck run Maxwell's healthchecks. Considered unhealthy if >0 messages have failed in the last 15 minutes. /ping a simple ping test, responds with pong JMX Configuration Standard configuration is either via commandline args or the config.properties file. However, when exposing JMX metrics additional configuration is required to allow remote access. In this case Maxwell makes use of the JAVA_OPTS environment variable. To make use of this set JAVA_OPTS before starting Maxwell. The following is an example which allows remote access with no authentication and insecure connections. export JAVA_OPTS=\"-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=9010 \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=SERVER_IP_ADDRESS\"","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"Maxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options or the config.properties file. These can provide insight into system health.","title":"Monitoring"},{"location":"monitoring/#metrics","text":"All metrics are prepended with the configured metrics_prefix. metric description Counters messages.succeeded count of messages that were successfully sent to Kafka messages.failed count of messages that failed to send to Kafka row.count a count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka. Meters messages.succeeded.meter a measure of the rate at which messages were successfully sent to Kafka messages.failed.meter a measure of the rate at which messages failed to send Kafka row.meter a measure of the rate at which rows arrive to Maxwell from the binlog connector Gauges replication.lag the time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds inflightmessages.count the number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are) Timers message.publish.time the time it took to send a given record to Kafka, in milliseconds message.publish.age the time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms. replication.queue.time the time it took to enqueue a given binlog event for processing, in milliseconds","title":"Metrics"},{"location":"monitoring/#http-endpoints","text":"When the HTTP server is enabled the following endpoints are exposed: endpoint description /metrics return all metrics as JSON /prometheus return all metrics as Prometheus format /healthcheck run Maxwell's healthchecks. Considered unhealthy if >0 messages have failed in the last 15 minutes. /ping a simple ping test, responds with pong","title":"HTTP Endpoints"},{"location":"monitoring/#jmx-configuration","text":"Standard configuration is either via commandline args or the config.properties file. However, when exposing JMX metrics additional configuration is required to allow remote access. In this case Maxwell makes use of the JAVA_OPTS environment variable. To make use of this set JAVA_OPTS before starting Maxwell. The following is an example which allows remote access with no authentication and insecure connections. export JAVA_OPTS=\"-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=9010 \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=SERVER_IP_ADDRESS\"","title":"JMX Configuration"},{"location":"producers/","text":"Kafka The Kafka producer is perhaps the most production hardened of all the producers, having run on high traffic instances at WEB scale. Topic Maxwell writes to a kafka topic named \"maxwell\" by default. It is configurable via --kafka_topic . The given topic can be a plain string or a dynamic string, e.g. namespace_%{database}_%{table} , where the topic will be generated from data in the row. Client version By default, maxwell uses the kafka 1.0.0 library. The --kafka_version flag lets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or 0.11.0.1, 1.0.0. This flag is only available on the command line. The 0.8.2.2 client is only compatible with brokers running kafka 0.8. The 0.10.0.x client is only compatible with brokers 0.10.0.x or later. Mixing the 0.10 client with other versions can lead to serious performance impacts. For More details, read about it here . The 0.11.0 client can talk to version 0.10.0 or newer brokers. The 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case: ERROR Sender - Uncaught error in kafka producer I/O thread: SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException Passing options to kafka Any options present in config.properties that are prefixed with kafka. will be passed into the Kafka producer library (with kafka. stripped off, see below for examples). We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs Highest throughput These properties would give high throughput performance. kafka.acks = 1 kafka.compression.type = snappy kafka.retries=0 Most reliable For at-least-once delivery, you will want something more like: kafka.acks = all kafka.retries = 5 # or some larger number And you will also want to set min.insync.replicas on Maxwell's output topic. Keys Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format: { \"database\":\"test_tb\",\"table\":\"test_tbl\",\"pk.id\":4,\"pk.part2\":\"hello\"} This key is designed to co-operate with Kafka's log compaction, which will save the last-known value for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act as a source of truth. Partitioning Both Kafka and AWS Kinesis support the notion of partitioned streams. Because they like to make our lives hard, Kafka calls its two units \"topics\" and \"partitions\", and Kinesis calls them \"streams\" and \"shards. They're the same thing, though. Maxwell is generally configured to write to N partitions/shards on one topic/stream, and how it distributes to those N partitions/shards can be controlled by producer_partition_by . producer_partition_by gives you a choice of splitting your stream by database, table, primary key, transaction id, column data, or \"random\". How you choose to partition your stream greatly influences the load and serialization properties of your downstream consumers, so choose carefully. A good rule of thumb is to use the finest-grained partition scheme possible given serialization constraints. A brief partitioning digression: If I were building, say, a simple search index of a single table, I might choose to partition by primary key; this would give you the best distribution of workload amongst your stream processors while maintaining a strict ordering of updates that happen to a certain row. If I were building something that needed better serialization properties -- let's say I needed to maintain strict ordering between updates that occured on different tables -- I would drop back to partitioning by table or database. This will reduce my throughput by a lot as a single stream consumer node will end up will all the load for particular table/database, but I'm guaranteed that the updates stay in order. If you choose to partition by column data (that is, values inside columns in your updates), you must set both: producer_partition_columns - a comma-separated list of column names, and producer_partiton_by_fallback - [ database , table , primary_key ] - this will be used as the partition key when the column does not exist. When partitioning by column Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data. Kafka partitioning A binlog event's partition is determined by the selected hash function and hash string as follows HASH_FUNCTION(producer_partion_value) % TOPIC.NUMBER_OF_PARTITIONS The HASH_FUNCTION is either java's hashCode or murmurhash3 . The default HASH_FUNCTION is hashCode . Murmurhash3 may be set with the kafka_partition_hash option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class. We tell you this in case you need to reverse engineer where a row will land. Maxwell will discover the number of partitions in its kafka topic upon boot. This means that you should pre-create your kafka topics: bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\ --topic maxwell --partitions 20 --replication-factor 2 http://kafka.apache.org/documentation.html#quickstart Kinesis AWS Credentials You will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to: \"kinesis:PutRecord\" \"kinesis:PutRecords\" \"kinesis:DescribeStream\" Additionally, the producer will need to be able to produce CloudWatch metrics which requires the following permission applied to the resource `*``: - \"cloudwatch:PutMetricData\" The resulting IAM policy document may look like this: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"kinesis:PutRecord\", \"kinesis:PutRecords\", \"kinesis:DescribeStream\" ], \"Resource\": \"arn:aws:kinesis:us-west-2:123456789012:stream/my-stream\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\" } ] } See the AWS docs for the latest examples on which permissions are needed. The producer uses the DefaultAWSCredentialsProviderChain class to gain aws credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. Options Set the output stream in config.properties by setting the kinesis_stream property. The producer uses the KPL (Kinesis Producer Library) and uses the KPL built in configurations. Copy kinesis-producer-library.properties.example to kinesis-producer-library.properties and configure the properties file to your needs. You are required to configure the region. For example: # set explicitly Region=us-west-2 # or set with an environment variable Region=$AWS_DEFAULT_REGION By default, the KPL implements record aggregation , which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the KCL (Kinesis Client Library) to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the AWS Kinesis Aggregation library ), or disable record aggregation in your kinesis-producer-library.properties configuration. To disable aggregation, add the following to your configuration: AggregationEnabled=false Remember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput. SQS AWS Credentials You will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs . Options Set the output queue in the config.properties by setting the sqs_queue_uri property to full SQS queue uri from AWS console. The producer uses the AWS SQS SDK . Nats The configurable properties for nats are: nats_url - defaults to nats://localhost:4222 nats_subject - defaults to %{database}.%{table} nats_subject defines the Nats subject hierarchy to write to. Topic substitution is available. All non-alphanumeric characters in the substitued values will be replaced by underscores. Google Cloud Pub/Sub In order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the roles/pubsub.publisher role. See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the pubsub_project_id and pubsub_topic properties. Optionally configure a dedicated output topic for DDL updates by setting the ddl_pubsub_topic property. The producer uses the Google Cloud Java Library for Pub/Sub and uses its built-in configurations. RabbitMQ To produce messages to RabbitMQ, you will need to specify a host in config.properties with rabbitmq_host . This is the only required property, everything else falls back to a sane default. The remaining configurable properties are: rabbitmq_user - defaults to guest rabbitmq_pass - defaults to guest rabbitmq_virtual_host - defaults to / rabbitmq_exchange - defaults to maxwell rabbitmq_exchange_type - defaults to fanout rabbitmq_exchange_durable - defaults to false rabbitmq_exchange_autodelete - defaults to false rabbitmq_routing_key_template - defaults to %db%.%table% This config controls the routing key, where %db% and %table% are placeholders that will be substituted at runtime rabbitmq_message_persistent - defaults to false rabbitmq_declare_exchange - defaults to true For more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html Redis Set the output stream in config.properties by setting the redis_type property to either pubsub , xadd , lpush or rpsuh . The redis_key is used as a channel for pubsub , as stream key for xadd and as key for lpush and rpush . Maxwell writes to a Redis channel named \"maxwell\" by default. It can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table} . In the latter case 'database' and 'table' will be replaced with the values for the row being processed. This can be changed with the redis_pub_channel , redis_list_key and redis_stream_key option. Other configurable properties are: redis_host - defaults to localhost redis_port - defaults to 6379 redis_auth - defaults to null redis_database - defaults to 0 redis_type - defaults to pubsub redis_key - defaults to maxwell redis_stream_json_key - defaults to message redis_sentinels - doesn't have a default value redis_sentinel_master_name - doesn't have a default value Custom Producer If none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. The producer is responsible for processing the raw database rows. Note that your producer may receive DDL and heartbeat rows as well, but your producer can easily filter them out (see example). In order to register your custom producer, you must implement the ProducerFactory interface, which is responsible for creating your custom AbstractProducer . Next, set the custom_producer.factory configuration property to your ProducerFactory 's fully qualified class name. Then add the custom ProducerFactory and all its dependencies to the $MAXWELL_HOME/lib directory. Your custom producer will likely require configuration properties as well. For that, use the custom_producer.* property namespace. Those properties will be exposed to your producer via MaxwellConfig.customProducerProperties . Custom producer factory and producer examples can be found here: https://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory Topic substitution Some producers may be given a template string from which they dynamically generate a topic (or whatever their equivalent of a kafka topic is). Subsitutions are enclosed in by %{} . The following substitutions are available: %{database} %{table} %{type} (insert/update/delete) Topic substituion is available in the following producers: Kakfa, for topics Redis, for channels Nats, for subject heirarchies","title":"Producers"},{"location":"producers/#kafka","text":"The Kafka producer is perhaps the most production hardened of all the producers, having run on high traffic instances at WEB scale.","title":"Kafka"},{"location":"producers/#topic","text":"Maxwell writes to a kafka topic named \"maxwell\" by default. It is configurable via --kafka_topic . The given topic can be a plain string or a dynamic string, e.g. namespace_%{database}_%{table} , where the topic will be generated from data in the row.","title":"Topic"},{"location":"producers/#client-version","text":"By default, maxwell uses the kafka 1.0.0 library. The --kafka_version flag lets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or 0.11.0.1, 1.0.0. This flag is only available on the command line. The 0.8.2.2 client is only compatible with brokers running kafka 0.8. The 0.10.0.x client is only compatible with brokers 0.10.0.x or later. Mixing the 0.10 client with other versions can lead to serious performance impacts. For More details, read about it here . The 0.11.0 client can talk to version 0.10.0 or newer brokers. The 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case: ERROR Sender - Uncaught error in kafka producer I/O thread: SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException","title":"Client version"},{"location":"producers/#passing-options-to-kafka","text":"Any options present in config.properties that are prefixed with kafka. will be passed into the Kafka producer library (with kafka. stripped off, see below for examples). We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs","title":"Passing options to kafka"},{"location":"producers/#highest-throughput","text":"These properties would give high throughput performance. kafka.acks = 1 kafka.compression.type = snappy kafka.retries=0","title":"Highest throughput"},{"location":"producers/#most-reliable","text":"For at-least-once delivery, you will want something more like: kafka.acks = all kafka.retries = 5 # or some larger number And you will also want to set min.insync.replicas on Maxwell's output topic.","title":"Most reliable"},{"location":"producers/#keys","text":"Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format: { \"database\":\"test_tb\",\"table\":\"test_tbl\",\"pk.id\":4,\"pk.part2\":\"hello\"} This key is designed to co-operate with Kafka's log compaction, which will save the last-known value for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act as a source of truth.","title":"Keys"},{"location":"producers/#partitioning","text":"Both Kafka and AWS Kinesis support the notion of partitioned streams. Because they like to make our lives hard, Kafka calls its two units \"topics\" and \"partitions\", and Kinesis calls them \"streams\" and \"shards. They're the same thing, though. Maxwell is generally configured to write to N partitions/shards on one topic/stream, and how it distributes to those N partitions/shards can be controlled by producer_partition_by . producer_partition_by gives you a choice of splitting your stream by database, table, primary key, transaction id, column data, or \"random\". How you choose to partition your stream greatly influences the load and serialization properties of your downstream consumers, so choose carefully. A good rule of thumb is to use the finest-grained partition scheme possible given serialization constraints. A brief partitioning digression: If I were building, say, a simple search index of a single table, I might choose to partition by primary key; this would give you the best distribution of workload amongst your stream processors while maintaining a strict ordering of updates that happen to a certain row. If I were building something that needed better serialization properties -- let's say I needed to maintain strict ordering between updates that occured on different tables -- I would drop back to partitioning by table or database. This will reduce my throughput by a lot as a single stream consumer node will end up will all the load for particular table/database, but I'm guaranteed that the updates stay in order. If you choose to partition by column data (that is, values inside columns in your updates), you must set both: producer_partition_columns - a comma-separated list of column names, and producer_partiton_by_fallback - [ database , table , primary_key ] - this will be used as the partition key when the column does not exist. When partitioning by column Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data.","title":"Partitioning"},{"location":"producers/#kafka-partitioning","text":"A binlog event's partition is determined by the selected hash function and hash string as follows HASH_FUNCTION(producer_partion_value) % TOPIC.NUMBER_OF_PARTITIONS The HASH_FUNCTION is either java's hashCode or murmurhash3 . The default HASH_FUNCTION is hashCode . Murmurhash3 may be set with the kafka_partition_hash option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class. We tell you this in case you need to reverse engineer where a row will land. Maxwell will discover the number of partitions in its kafka topic upon boot. This means that you should pre-create your kafka topics: bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\ --topic maxwell --partitions 20 --replication-factor 2 http://kafka.apache.org/documentation.html#quickstart","title":"Kafka partitioning"},{"location":"producers/#kinesis","text":"","title":"Kinesis"},{"location":"producers/#aws-credentials","text":"You will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to: \"kinesis:PutRecord\" \"kinesis:PutRecords\" \"kinesis:DescribeStream\" Additionally, the producer will need to be able to produce CloudWatch metrics which requires the following permission applied to the resource `*``: - \"cloudwatch:PutMetricData\" The resulting IAM policy document may look like this: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"kinesis:PutRecord\", \"kinesis:PutRecords\", \"kinesis:DescribeStream\" ], \"Resource\": \"arn:aws:kinesis:us-west-2:123456789012:stream/my-stream\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\" } ] } See the AWS docs for the latest examples on which permissions are needed. The producer uses the DefaultAWSCredentialsProviderChain class to gain aws credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain.","title":"AWS Credentials"},{"location":"producers/#options","text":"Set the output stream in config.properties by setting the kinesis_stream property. The producer uses the KPL (Kinesis Producer Library) and uses the KPL built in configurations. Copy kinesis-producer-library.properties.example to kinesis-producer-library.properties and configure the properties file to your needs. You are required to configure the region. For example: # set explicitly Region=us-west-2 # or set with an environment variable Region=$AWS_DEFAULT_REGION By default, the KPL implements record aggregation , which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the KCL (Kinesis Client Library) to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the AWS Kinesis Aggregation library ), or disable record aggregation in your kinesis-producer-library.properties configuration. To disable aggregation, add the following to your configuration: AggregationEnabled=false Remember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput.","title":"Options"},{"location":"producers/#sqs","text":"","title":"SQS"},{"location":"producers/#aws-credentials_1","text":"You will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs .","title":"AWS Credentials"},{"location":"producers/#options_1","text":"Set the output queue in the config.properties by setting the sqs_queue_uri property to full SQS queue uri from AWS console. The producer uses the AWS SQS SDK .","title":"Options"},{"location":"producers/#nats","text":"The configurable properties for nats are: nats_url - defaults to nats://localhost:4222 nats_subject - defaults to %{database}.%{table} nats_subject defines the Nats subject hierarchy to write to. Topic substitution is available. All non-alphanumeric characters in the substitued values will be replaced by underscores.","title":"Nats"},{"location":"producers/#google-cloud-pubsub","text":"In order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the roles/pubsub.publisher role. See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the pubsub_project_id and pubsub_topic properties. Optionally configure a dedicated output topic for DDL updates by setting the ddl_pubsub_topic property. The producer uses the Google Cloud Java Library for Pub/Sub and uses its built-in configurations.","title":"Google Cloud Pub/Sub"},{"location":"producers/#rabbitmq","text":"To produce messages to RabbitMQ, you will need to specify a host in config.properties with rabbitmq_host . This is the only required property, everything else falls back to a sane default. The remaining configurable properties are: rabbitmq_user - defaults to guest rabbitmq_pass - defaults to guest rabbitmq_virtual_host - defaults to / rabbitmq_exchange - defaults to maxwell rabbitmq_exchange_type - defaults to fanout rabbitmq_exchange_durable - defaults to false rabbitmq_exchange_autodelete - defaults to false rabbitmq_routing_key_template - defaults to %db%.%table% This config controls the routing key, where %db% and %table% are placeholders that will be substituted at runtime rabbitmq_message_persistent - defaults to false rabbitmq_declare_exchange - defaults to true For more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html","title":"RabbitMQ"},{"location":"producers/#redis","text":"Set the output stream in config.properties by setting the redis_type property to either pubsub , xadd , lpush or rpsuh . The redis_key is used as a channel for pubsub , as stream key for xadd and as key for lpush and rpush . Maxwell writes to a Redis channel named \"maxwell\" by default. It can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table} . In the latter case 'database' and 'table' will be replaced with the values for the row being processed. This can be changed with the redis_pub_channel , redis_list_key and redis_stream_key option. Other configurable properties are: redis_host - defaults to localhost redis_port - defaults to 6379 redis_auth - defaults to null redis_database - defaults to 0 redis_type - defaults to pubsub redis_key - defaults to maxwell redis_stream_json_key - defaults to message redis_sentinels - doesn't have a default value redis_sentinel_master_name - doesn't have a default value","title":"Redis"},{"location":"producers/#custom-producer","text":"If none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. The producer is responsible for processing the raw database rows. Note that your producer may receive DDL and heartbeat rows as well, but your producer can easily filter them out (see example). In order to register your custom producer, you must implement the ProducerFactory interface, which is responsible for creating your custom AbstractProducer . Next, set the custom_producer.factory configuration property to your ProducerFactory 's fully qualified class name. Then add the custom ProducerFactory and all its dependencies to the $MAXWELL_HOME/lib directory. Your custom producer will likely require configuration properties as well. For that, use the custom_producer.* property namespace. Those properties will be exposed to your producer via MaxwellConfig.customProducerProperties . Custom producer factory and producer examples can be found here: https://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory","title":"Custom Producer"},{"location":"producers/#topic-substitution","text":"Some producers may be given a template string from which they dynamically generate a topic (or whatever their equivalent of a kafka topic is). Subsitutions are enclosed in by %{} . The following substitutions are available: %{database} %{table} %{type} (insert/update/delete) Topic substituion is available in the following producers: Kakfa, for topics Redis, for channels Nats, for subject heirarchies","title":"Topic substitution"},{"location":"quickstart/","text":"Download Download binary distro: https://github.com/zendesk/maxwell/releases/download/v1.31.0/maxwell-1.31.0.tar.gz Sources and bug tracking is available on github: https://github.com/zendesk/maxwell curl : curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.31.0/maxwell-1.31.0.tar.gz \\ | tar zxvf - cd maxwell-1.31.0 docker : docker pull zendesk/maxwell homebrew : brew install maxwell Configure Mysql Server Config: Ensure server_id is set, and that row-based replication is on. $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row Or on a running server: mysql> set global binlog_format=ROW; mysql> set global binlog_row_image=FULL; note: binlog_format is a session-based property. You will need to shutdown all active connections to fully convert to row-based replication. Permissions: Maxwell needs permissions to act as a replica, and to write to the maxwell database. mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%'; # or for running maxwell locally: mysql> CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'localhost'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost'; Run Maxwell Command line bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout Docker docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout Kafka Boot kafka as described here: http://kafka.apache.org/documentation.html#quickstart , then: bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell (or docker): docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\ --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell Kinesis docker run -it --rm --name maxwell -v `cd && pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties && echo \"Region=$AWS_DEFAULT_REGION\" >> /app/kinesis-producer-library.properties && bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM' Nats bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=nats --nats_url=='0.0.0.0:4222' Google Cloud Pub/Sub bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\ --pubsub_topic='maxwell' RabbitMQ bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname' Redis bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=redis --redis_host=redis.hostname","title":"Quick Start"},{"location":"quickstart/#download","text":"Download binary distro: https://github.com/zendesk/maxwell/releases/download/v1.31.0/maxwell-1.31.0.tar.gz Sources and bug tracking is available on github: https://github.com/zendesk/maxwell curl : curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.31.0/maxwell-1.31.0.tar.gz \\ | tar zxvf - cd maxwell-1.31.0 docker : docker pull zendesk/maxwell homebrew : brew install maxwell","title":"Download"},{"location":"quickstart/#configure-mysql","text":"Server Config: Ensure server_id is set, and that row-based replication is on. $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row Or on a running server: mysql> set global binlog_format=ROW; mysql> set global binlog_row_image=FULL; note: binlog_format is a session-based property. You will need to shutdown all active connections to fully convert to row-based replication. Permissions: Maxwell needs permissions to act as a replica, and to write to the maxwell database. mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%'; # or for running maxwell locally: mysql> CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'localhost'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost';","title":"Configure Mysql"},{"location":"quickstart/#run-maxwell","text":"","title":"Run Maxwell"},{"location":"quickstart/#command-line","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout","title":"Command line"},{"location":"quickstart/#docker","text":"docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout","title":"Docker"},{"location":"quickstart/#kafka","text":"Boot kafka as described here: http://kafka.apache.org/documentation.html#quickstart , then: bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell (or docker): docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\ --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell","title":"Kafka"},{"location":"quickstart/#kinesis","text":"docker run -it --rm --name maxwell -v `cd && pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties && echo \"Region=$AWS_DEFAULT_REGION\" >> /app/kinesis-producer-library.properties && bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM'","title":"Kinesis"},{"location":"quickstart/#nats","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=nats --nats_url=='0.0.0.0:4222'","title":"Nats"},{"location":"quickstart/#google-cloud-pubsub","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\ --pubsub_topic='maxwell'","title":"Google Cloud Pub/Sub"},{"location":"quickstart/#rabbitmq","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname'","title":"RabbitMQ"},{"location":"quickstart/#redis","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=redis --redis_host=redis.hostname","title":"Redis"},{"location":"schemas/","text":"Internals Schema storage MySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc. There is a set of \"base schema\" tables in the maxwell database - tables , columns , databases . This is where we capture the initial schema, the first time Maxwell is run. As time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the schemas table - each diff contains the following information to place it in the timeline of your database: binlog_file , binlog_position (or gtid_set ): the exact point in the binlog stream where the schema change occurred deltas : the internal representation of the schema change base_schema_id : the previous schema that this delta applies to last_heartbeat_read : the most recent maxwell heartbeat seen in the binlog prior to this change server_id : the server which this schema applies to This information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of base_schema_id until it terminates (in a null , which means we've reached the initial captured schema). \"Most recent\" should be fairly intuitive - firstly we sort by last_heartbeat_read , then by binlog_file , then binlog_position . We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position. Master failover Schemas are important for master failover. When Maxwell detects that it is talking to a new server_id (one that differs from its stored position ), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to maxwell.heartbeats corresponding to the timestamp stored in its position table. Once it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event. Using this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new server_id , and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.","title":"Internals"},{"location":"schemas/#internals","text":"","title":"Internals"},{"location":"schemas/#schema-storage","text":"MySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc. There is a set of \"base schema\" tables in the maxwell database - tables , columns , databases . This is where we capture the initial schema, the first time Maxwell is run. As time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the schemas table - each diff contains the following information to place it in the timeline of your database: binlog_file , binlog_position (or gtid_set ): the exact point in the binlog stream where the schema change occurred deltas : the internal representation of the schema change base_schema_id : the previous schema that this delta applies to last_heartbeat_read : the most recent maxwell heartbeat seen in the binlog prior to this change server_id : the server which this schema applies to This information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of base_schema_id until it terminates (in a null , which means we've reached the initial captured schema). \"Most recent\" should be fairly intuitive - firstly we sort by last_heartbeat_read , then by binlog_file , then binlog_position . We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position.","title":"Schema storage"},{"location":"schemas/#master-failover","text":"Schemas are important for master failover. When Maxwell detects that it is talking to a new server_id (one that differs from its stored position ), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to maxwell.heartbeats corresponding to the timestamp stored in its position table. Once it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event. Using this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new server_id , and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.","title":"Master failover"}]}